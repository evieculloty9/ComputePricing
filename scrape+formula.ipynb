{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# single source of truth for dashboard outputs\n",
    "OUTPUT_DIR = Path(\"docs/data/derived\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_csv(df, name):\n",
    "    p = OUTPUT_DIR / name\n",
    "    df.to_csv(p, index=False)\n",
    "    print(f\"✔ saved: {p}\")\n",
    "\n",
    "# …then everywhere you save:\n",
    "# save_csv(provider_scores_df, \"provider_scores_latest.csv\")\n",
    "# save_csv(roi_df,              \"roi_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape provider\n",
    "# --- Async helper that works in notebooks and GitHub Actions ---\n",
    "import asyncio\n",
    "\n",
    "def await_safe(coro):\n",
    "    \"\"\"\n",
    "    Run an async coroutine from anywhere:\n",
    "    - If an event loop is already running (Jupyter/nbconvert), use nest_asyncio + run_until_complete\n",
    "    - Otherwise, use asyncio.run\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            try:\n",
    "                import nest_asyncio\n",
    "                nest_asyncio.apply()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # No current loop\n",
    "        return asyncio.run(coro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>8X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-10 12:21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>4X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>4</td>\n",
       "      <td>3.09</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-10 12:21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>2X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-10 12:21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-10 12:21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>3.29</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-10 12:21:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region    gpu_model       type duration  gpu_count  \\\n",
       "0  Lambda Labs     US  8X H100 SXM  On-Demand       1h          8   \n",
       "1  Lambda Labs     US  4X H100 SXM  On-Demand       1h          4   \n",
       "2  Lambda Labs     US  2X H100 SXM  On-Demand       1h          2   \n",
       "3  Lambda Labs     US      1X H200  On-Demand       1h          1   \n",
       "4  Lambda Labs     US  1X H100 SXM  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                            source_url      fetched_at_utc  \n",
       "0              2.99  https://cloud.lambdalabs.com/pricing 2025-09-10 12:21:30  \n",
       "1              3.09  https://cloud.lambdalabs.com/pricing 2025-09-10 12:21:30  \n",
       "2              3.19  https://cloud.lambdalabs.com/pricing 2025-09-10 12:21:30  \n",
       "3              1.49  https://cloud.lambdalabs.com/pricing 2025-09-10 12:21:30  \n",
       "4              3.29  https://cloud.lambdalabs.com/pricing 2025-09-10 12:21:30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lambda Labs \n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _norm_gpu(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")  # treat GH200 as H200\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(s: str) -> Optional[int]:\n",
    "    if not isinstance(s, str): \n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)x\", s, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_in(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes https://cloud.lambdalabs.com/pricing and returns SLIM rows\n",
    "    for H100/H200 (On-Demand, 1h). If `region` provided, overrides detected region.\n",
    "    \"\"\"\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    rows_out = []\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        tbl_region = region or _infer_region(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            row_text = \" | \".join(tds)\n",
    "\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "\n",
    "            price = _price_in(row_text)\n",
    "            if price is None:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            gpu_model = _norm_gpu(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            count = _gpu_count(gpu_model)\n",
    "\n",
    "            rows_out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": gpu_model,      \n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": count,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    df = df[keep].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# Example:\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "display(df_lambda.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sfcompute] history  -> docs/data/history/sfcompute_history.csv\n",
      "[sfcompute] latest   -> docs/data/latest/sfcompute_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFCompute</td>\n",
       "      <td>US</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>https://www.sfcompute.com</td>\n",
       "      <td>2025-09-10T12:21:51+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SFCompute</td>\n",
       "      <td>US</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>https://www.sfcompute.com</td>\n",
       "      <td>2025-09-10T12:21:51+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider region gpu_model       type duration  gpu_count  \\\n",
       "0  SFCompute     US      H200  On-Demand       1h          1   \n",
       "1  SFCompute     US      H200  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                 source_url             fetched_at_utc  \n",
       "0              1.38  https://www.sfcompute.com  2025-09-10T12:21:51+00:00  \n",
       "1              1.40  https://www.sfcompute.com  2025-09-10T12:21:51+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= SFCompute — H100/H200 scraper (slim schema) =================\n",
    "# Output files:\n",
    "#   docs/data/history/sfcompute_history.csv\n",
    "#   docs/data/latest/sfcompute_latest.csv\n",
    "#\n",
    "# Notes:\n",
    "# - Tries to capture *per GPU-hour* prices directly (e.g. \"per GPU hour\", \"per card-hour\").\n",
    "# - If only instance price is shown with \"x GPUs\", we divide by the parsed GPU count.\n",
    "# - Very defensive: regex-only parsing; ignores out-of-band prices.\n",
    "# - Region defaults to \"Global\" (adjust mapping below if their page shows regions).\n",
    "#\n",
    "# Requires: pandas, playwright (already in your CI), requests, bs4 (optional)\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# --------------------- storage + schema helpers ---------------------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, LATEST_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    # history append + dedupe\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"price_hourly_usd\",\"fetched_at_utc\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_out = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_out = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "\n",
    "    print(f\"[sfcompute] history  -> {hist_out}\")\n",
    "    print(f\"[sfcompute] latest   -> {latest_out}\")\n",
    "    return latest\n",
    "\n",
    "# --------------------- scraping patterns ---------------------\n",
    "GPU_RE = r\"(H100|H200)\\b\"\n",
    "USD_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "USD_PER_GPU_HR = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:gpu|card)[-\\s]?(?:h|hr|hour)\\b\", re.I)\n",
    "COUNT_PATS = [\n",
    "    re.compile(r\"(\\d+)\\s*[×x]\\s*(?:GPUs?|cards?)\\b\", re.I), # \"8x GPUs\"\n",
    "    re.compile(r\"(?:GPUs?|cards?)\\s*[:\\-]?\\s*(\\d+)\\b\", re.I), # \"GPUs: 8\"\n",
    "    re.compile(r\"\\b(\\d+)\\s*[×x]\\b\", re.I),                  # \"8x\"\n",
    "]\n",
    "MODEL_NEAR_PRICE = re.compile(rf\"{GPU_RE}.*?\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", re.I | re.S)\n",
    "\n",
    "def _parse_count(text: str):\n",
    "    for pat in COUNT_PATS:\n",
    "        m = pat.search(text)\n",
    "        if m:\n",
    "            try:\n",
    "                n = int(m.group(1))\n",
    "                if 1 <= n <= 64:\n",
    "                    return n\n",
    "            except: \n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def _detect_region(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    if \"us\" in t or \"usa\" in t or \"america\" in t: return \"US\"\n",
    "    if \"europe\" in t or \"eu \" in t or \" eu-\" in t: return \"EU\"\n",
    "    if \"asia\" in t or \"apac\" in t: return \"APAC\"\n",
    "    return \"Global\"\n",
    "\n",
    "async def _fetch_text_playwright(url: str, wait: str = \"domcontentloaded\") -> str:\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=wait)\n",
    "        # light scroll to trigger lazy content\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(600)\n",
    "        body = await page.content()\n",
    "        await browser.close()\n",
    "    # get text from HTML\n",
    "    soup = BeautifulSoup(body, \"html.parser\")\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def _fetch_text_requests(url: str) -> str:\n",
    "    html = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=60).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "# --------------------- main scrape ---------------------\n",
    "async def scrape_sfcompute(urls=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tries a list of candidate pages and extracts H100/H200 prices.\n",
    "    Adjust `urls` if SFCompute hosts pricing elsewhere.\n",
    "    \"\"\"\n",
    "    if urls is None:\n",
    "        urls = [\n",
    "            \"https://sfcompute.com\",              # root\n",
    "            \"https://sfcompute.com/pricing\",      # common pattern\n",
    "            \"https://www.sfcompute.com/pricing\",\n",
    "            \"https://www.sfcompute.com\"           # fallback\n",
    "        ]\n",
    "\n",
    "    rows = []\n",
    "    fetched_any = False\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # try Playwright first (handles dynamic sites), fallback to requests\n",
    "            try:\n",
    "                text = await _fetch_text_playwright(url)\n",
    "            except Exception:\n",
    "                text = _fetch_text_requests(url)\n",
    "\n",
    "            fetched_any = True\n",
    "            norm = re.sub(r\"\\s+\", \" \", text)\n",
    "            up = norm.upper()\n",
    "\n",
    "            # quick region hint from the page (very rough)\n",
    "            region = _detect_region(norm)\n",
    "\n",
    "            # 1) Direct per-GPU/hour prices\n",
    "            for m in re.finditer(rf\"{GPU_RE}.*?{USD_PER_GPU_HR.pattern}\", norm, flags=re.I):\n",
    "                seg = m.group(0)\n",
    "                gm = re.search(GPU_RE, seg, flags=re.I)\n",
    "                pm = re.search(USD_PER_GPU_HR, seg, flags=re.I)\n",
    "                if not (gm and pm): \n",
    "                    continue\n",
    "                model = gm.group(1).upper()\n",
    "                price = float(pm.group(1))\n",
    "                if not (0.2 <= price <= 50.0): \n",
    "                    continue\n",
    "                rows.append({\n",
    "                    \"provider\": \"SFCompute\",\n",
    "                    \"region\": region,\n",
    "                    \"gpu_model\": model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": 1,\n",
    "                    \"price_hourly_usd\": price,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "\n",
    "            # 2) Instance price + GPU count nearby → per-GPU\n",
    "            #    Look for \"... H100 ... $X /hour ... GPUs: N ...\"\n",
    "            for m in re.finditer(MODEL_NEAR_PRICE, norm):\n",
    "                model = m.group(1).upper()\n",
    "                instance_price = float(m.group(2))\n",
    "                # sanity on instance price band\n",
    "                if not (0.5 <= instance_price <= 1000):\n",
    "                    continue\n",
    "                # search a local window around the match for counts\n",
    "                start = max(0, m.start() - 120)\n",
    "                end   = min(len(norm), m.end() + 120)\n",
    "                window = norm[start:end]\n",
    "                cnt = _parse_count(window) or 1\n",
    "                per_gpu = instance_price / max(1, cnt)\n",
    "                if not (0.2 <= per_gpu <= 50.0): \n",
    "                    continue\n",
    "                rows.append({\n",
    "                    \"provider\": \"SFCompute\",\n",
    "                    \"region\": region,\n",
    "                    \"gpu_model\": model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": int(cnt),\n",
    "                    \"price_hourly_usd\": round(per_gpu, 4),\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] sfcompute fetch failed for {url}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty and fetched_any:\n",
    "        # create empty slim to keep pipeline happy\n",
    "        df = _ensure_slim(pd.DataFrame([], columns=SLIM_COLS))\n",
    "    elif not df.empty:\n",
    "        # de-dupe by (model, price)\n",
    "        df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "                .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "                .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# --------------------- run & save ---------------------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "df_sf = arun(scrape_sfcompute())\n",
    "latest_sf = _save_provider(df_sf, \"sfcompute\")\n",
    "display(df_sf.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lambda_labs] snapshot -> docs/data/snapshots/20250910_122153_lambda_labs.csv\n",
      "[lambda_labs] history  -> docs/data/history/lambda_labs_history.csv\n",
      "[lambda_labs] latest   -> docs/data/latest/lambda_labs_latest.csv\n",
      "[runpod] snapshot -> docs/data/snapshots/20250910_122156_runpod.csv\n",
      "[runpod] history  -> docs/data/history/runpod_history.csv\n",
      "[runpod] latest   -> docs/data/latest/runpod_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200 141 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>3.59</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-10 12:21:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100 PCIE 80 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-10 12:21:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region        gpu_model       type duration gpu_count  \\\n",
       "6   RunPod  Global      H200 141 GB  On-Demand       1h      None   \n",
       "8   RunPod  Global  H100 PCIE 80 GB  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                     source_url      fetched_at_utc  \n",
       "6              3.59  https://www.runpod.io/pricing 2025-09-10 12:21:56  \n",
       "8              1.99  https://www.runpod.io/pricing 2025-09-10 12:21:56  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Lambda Labs (static) + RunPod (async) with per-provider history =====\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, os, asyncio, pandas as pd, tempfile\n",
    "from typing import Optional, Dict, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "# -------- storage (per-provider history/snapshots) --------\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        # fallback to tmp if workspace is read-only\n",
    "        tmp = Path(tempfile.gettempdir()) / \"gpu_data\"\n",
    "        d = tmp / d.name\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    try:\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    except Exception:\n",
    "        snap_path = Path(tempfile.gettempdir()) / f\"{ts}_{provider_slug}.csv\"\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    try:\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    except Exception:\n",
    "        hist_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_history.csv\"\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    # latest (newest rows only per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    try:\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    except Exception:\n",
    "        latest_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_latest.csv\"\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ------------------------- Lambda Labs (static) -------------------------\n",
    "def _norm_gpu_lambda(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"(\\d+)\\s*x\", text, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_dollar(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region_lambda(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30); r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    out = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        tbl_region = region or _infer_region_lambda(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds: continue\n",
    "            row_text = \" | \".join(tds)\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "            price = _price_dollar(row_text)\n",
    "            if price is None: continue\n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            model = _norm_gpu_lambda(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": _gpu_count(model),\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    return _ensure_slim(df[keep].reset_index(drop=True))\n",
    "\n",
    "# --------------------------- RunPod (async) ---------------------------\n",
    "def _extract_gpu_model_runpod(text: str) -> Optional[str]:\n",
    "    if not isinstance(text, str): return None\n",
    "    text_up = re.sub(r\"\\s+\", \" \", text.upper())\n",
    "    m = re.search(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", text_up)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _price_hourly_runpod(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    t = text.replace(\",\", \"\")\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", t, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "async def scrape_runpod_async() -> pd.DataFrame:\n",
    "    from playwright.async_api import async_playwright\n",
    "    url = \"https://www.runpod.io/pricing\"; region = \"Global\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1200)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\"], class_=re.compile(r\"(price|pricing|card|grid|table)\", re.I))\n",
    "    if not nodes:\n",
    "        nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\",\"p\",\"span\"])\n",
    "\n",
    "    out = []\n",
    "    for n in nodes:\n",
    "        text = n.get_text(\" \", strip=True)\n",
    "        if \"H100\" not in text and \"H200\" not in text: \n",
    "            continue\n",
    "        price = _price_hourly_runpod(text)\n",
    "        if price is None:\n",
    "            continue\n",
    "        model = _extract_gpu_model_runpod(text)\n",
    "        if model is None:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"provider\": \"RunPod\",\n",
    "            \"region\": region,\n",
    "            \"gpu_model\": model,\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": _gpu_count(text),\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    df = df[df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", na=False)]\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# --------------- Runner that works in scripts & notebooks ---------------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "# Lambda Labs\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "_save_provider(df_lambda, \"lambda_labs\")\n",
    "\n",
    "# RunPod\n",
    "df_runpod = arun(scrape_runpod_async())\n",
    "_save_provider(df_runpod, \"runpod\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nebius] snapshot -> docs/data/snapshots/20250910_122157_nebius.csv\n",
      "[nebius] history  -> docs/data/history/nebius_history.csv\n",
      "[nebius] latest   -> docs/data/latest/nebius_latest.csv\n",
      "Nebius rows this run: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-10T12:21:57.425341+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-10T12:21:57.425341+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0   Nebius  Global      H200  On-Demand       1h      None               2.3   \n",
       "1   Nebius  Global      H100  On-Demand       1h      None               2.0   \n",
       "\n",
       "                  source_url                    fetched_at_utc  \n",
       "0  https://nebius.com/prices  2025-09-10T12:21:57.425341+00:00  \n",
       "1  https://nebius.com/prices  2025-09-10T12:21:57.425341+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Nebius H100/H200 scraper (uses YOUR parsing + per-provider history) ---\n",
    "\n",
    "import re, time, tempfile, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "\n",
    "# ---------- your original config ----------\n",
    "url = \"https://nebius.com/prices\"\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\"}\n",
    "TZ = pytz.utc\n",
    "MIN_PRICE, MAX_PRICE = 0.3, 20.0   # sanity for $/GPU/hr\n",
    "\n",
    "# ---------- storage dirs (snapshot/history/latest) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # we'll fall back to tmp if write fails later\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(TZ).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- your original parsing (unchanged) ----------\n",
    "def find_price_strict(text: str):\n",
    "    \"\"\"Match $X/hr, $X per hour, $X/hour, case-insensitive.\"\"\"\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", text, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_nebius_from_html(html: str) -> dict:\n",
    "    \"\"\"Return {'H100': price, 'H200': price} if found.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = {}\n",
    "\n",
    "    # Focus on plausible pricing containers first (tables / pricing sections)\n",
    "    blocks = []\n",
    "    blocks.extend(soup.find_all(\"table\"))\n",
    "    if not blocks:\n",
    "        blocks.extend(soup.find_all([\"section\",\"div\"], class_=re.compile(\"price|pricing|compute\", re.I)))\n",
    "    if not blocks:\n",
    "        blocks = soup.find_all([\"div\",\"tr\",\"li\",\"p\",\"span\"])\n",
    "\n",
    "    for blk in blocks:\n",
    "        t = blk.get_text(\" \", strip=True)\n",
    "        if not t: continue\n",
    "\n",
    "        has_h100 = bool(re.search(r\"\\bH100\\b\", t, flags=re.I))\n",
    "        has_h200 = bool(re.search(r\"\\bH200\\b\", t, flags=re.I))\n",
    "        if not (has_h100 or has_h200): continue\n",
    "\n",
    "        price = find_price_strict(t)\n",
    "        if price is None or not (MIN_PRICE <= price <= MAX_PRICE): continue\n",
    "\n",
    "        if has_h100 and \"H100\" not in results:\n",
    "            results[\"H100\"] = price\n",
    "        if has_h200 and \"H200\" not in results:\n",
    "            results[\"H200\"] = price\n",
    "\n",
    "        if len(results) == 2:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------- run (your flow) ----------\n",
    "html = None\n",
    "try:\n",
    "    r = requests.get(url, headers=UA, timeout=30)\n",
    "    if r.status_code == 200 and r.text:\n",
    "        html = r.text\n",
    "except Exception:\n",
    "    html = None\n",
    "\n",
    "results = {}\n",
    "if html:\n",
    "    results = parse_nebius_from_html(html)\n",
    "\n",
    "# Optional Playwright fallback if nothing found\n",
    "if not results:\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "            page.wait_for_timeout(2000)  # allow dynamic content\n",
    "            html_pw = page.content()\n",
    "            browser.close()\n",
    "        results = parse_nebius_from_html(html_pw)\n",
    "    except Exception:\n",
    "        pass  # proceed with whatever we have\n",
    "\n",
    "# ---------- map YOUR results -> SLIM schema & save ----------\n",
    "rows = []\n",
    "ts = datetime.now(TZ).isoformat()\n",
    "for gpu, price in results.items():\n",
    "    rows.append({\n",
    "        \"provider\": \"Nebius\",\n",
    "        \"region\": \"Global\",            # keep simple; refine if you later detect regions\n",
    "        \"gpu_model\": gpu,              # map gpu_type -> gpu_model\n",
    "        \"type\": \"On-Demand\",\n",
    "        \"duration\": \"1h\",\n",
    "        \"gpu_count\": None,\n",
    "        \"price_hourly_usd\": price,     # map on_demand_price -> price_hourly_usd\n",
    "        \"source_url\": url,\n",
    "        \"fetched_at_utc\": ts,          # map scraped_at -> fetched_at_utc\n",
    "    })\n",
    "\n",
    "df_nebius = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "latest = _save_provider(df_nebius, \"nebius\")\n",
    "print(f\"Nebius rows this run: {len(df_nebius)}\")\n",
    "display(df_nebius.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voltagepark] snapshot -> docs/data/snapshots/20250910_122207_voltagepark.csv\n",
      "[voltagepark] history  -> docs/data/history/voltagepark_history.csv\n",
      "[voltagepark] latest   -> docs/data/latest/voltagepark_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-10 12:22:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region gpu_model       type duration gpu_count  \\\n",
       "0  VoltagePark     US      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                                         source_url  \\\n",
       "0              1.99  https://dashboard.voltagepark.com/order/config...   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-10 12:22:07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= VoltagePark (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, tempfile, pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage + schema helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- YOUR scraping logic, adapted to slim schema ----\n",
    "async def scrape_voltagepark() -> pd.DataFrame:\n",
    "    url = \"https://dashboard.voltagepark.com/order/configure-deployment\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_timeout(5000)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    for line in html.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                # your original pattern\n",
    "                m = re.search(r\"\\$?(\\d+(?:\\.\\d+)?)(?=/GPU/hour)\", line)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    gpu = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                    rows.append({\n",
    "                        \"provider\": \"VoltagePark\",\n",
    "                        \"region\": \"US\",\n",
    "                        \"gpu_model\": gpu,\n",
    "                        \"type\": \"On-Demand\",\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"gpu_count\": None,\n",
    "                        \"price_hourly_usd\": price,\n",
    "                        \"source_url\": url,\n",
    "                        \"fetched_at_utc\": _now_iso(),\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # keep silent in prod; print minimal context if you want\n",
    "                # print(f\"[VoltagePark Parse Error] {e}\")\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # dedupe by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # sanity: plausible $/hr range\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in notebooks & scripts ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_voltage = arun(scrape_voltagepark())\n",
    "_save_provider(df_voltage, \"voltagepark\")\n",
    "display(df_voltage.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vastai] snapshot -> docs/data/snapshots/20250910_122212_vastai.csv\n",
      "[vastai] history  -> docs/data/history/vastai_history.csv\n",
      "[vastai] latest   -> docs/data/latest/vastai_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-10 12:22:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  Vast.ai  Global      H100  On-Demand       1h      None              1.25   \n",
       "\n",
       "                           source_url      fetched_at_utc  \n",
       "0  https://vast.ai/products/gpu-cloud 2025-09-10 12:22:12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Vast.ai (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled below\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- YOUR scraping logic, adapted to slim schema --------\n",
    "async def scrape_vast_products() -> pd.DataFrame:\n",
    "    url = \"https://vast.ai/products/gpu-cloud\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        # Try to reveal lazy content\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1500)\n",
    "        await page.evaluate(\"window.scrollTo(0, 0)\")\n",
    "        await page.wait_for_timeout(500)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Your original approach: scan lines and pick $ numbers near H100/H200\n",
    "    for line in content.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                gpu_model = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                # pull all $-bearing tokens in the line\n",
    "                dollars = [s for s in re.split(r\"\\s+\", line) if \"$\" in s]\n",
    "                price_val = None\n",
    "                for token in dollars:\n",
    "                    clean = \"\".join(c for c in token if c.isdigit() or c == \".\")\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    price = float(clean)\n",
    "                    if 0.1 < price < 100:  # sanity filter like you had\n",
    "                        price_val = price\n",
    "                        break\n",
    "                if price_val is None:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"Vast.ai\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": gpu_model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": None,\n",
    "                    \"price_hourly_usd\": price_val,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "            except Exception:\n",
    "                # swallow parse errors to keep the run clean\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # Sanity clamp\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "df_vastp = arun(scrape_vast_products())\n",
    "_save_provider(df_vastp, \"vastai\")\n",
    "display(df_vastp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-10 12:22:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.35</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-10 12:22:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "1  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                 source_url      fetched_at_utc  \n",
       "0              1.99  https://www.shadeform.ai/ 2025-09-10 12:22:14  \n",
       "1              2.35  https://www.shadeform.ai/ 2025-09-10 12:22:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Shadeform: precise matcher (nearest-price + hourly hint) ====\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# slim schema storage helpers (use the same ones you already have)\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "# --------- robust extractors ----------\n",
    "# require an hourly hint, allowing variants like \"/GPU/hour\"\n",
    "PRICE_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/GPU)?\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_TOKENS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    # include B200 so we don't steal its prices\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|H800|A100|A800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_token_positions(text: str):\n",
    "    positions = {k: [] for k in GPU_TOKENS.keys()}\n",
    "    for name, pat in GPU_TOKENS.items():\n",
    "        for m in pat.finditer(text):\n",
    "            positions[name].append(m.start())\n",
    "    return positions\n",
    "\n",
    "def _find_price_positions(text: str):\n",
    "    return [(float(m.group(1)), m.start()) for m in PRICE_RE.finditer(text)]\n",
    "\n",
    "def _nearest_price_to_token(text: str, token: str, window: int = 220):\n",
    "    \"\"\"Yield (model, price) pairs by attaching each token occurrence\n",
    "       to the nearest price with an hourly hint, only if it is closer\n",
    "       to this token than to any other GPU token.\"\"\"\n",
    "    tok_positions = _find_token_positions(text)\n",
    "    prices = _find_price_positions(text)\n",
    "    if not tok_positions.get(token) or not prices:\n",
    "        return []\n",
    "\n",
    "    # all GPU-ish positions (to compete for 'closeness')\n",
    "    competitor_positions = []\n",
    "    for k, pos_list in tok_positions.items():\n",
    "        if k == token:  # we compare against others later\n",
    "            continue\n",
    "        competitor_positions.extend(pos_list)\n",
    "\n",
    "    rows = []\n",
    "    for gpos in tok_positions[token]:\n",
    "        # candidates within a window around the GPU string\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands:\n",
    "            continue\n",
    "        # pick nearest price to this token\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "\n",
    "        # ensure this price isn't actually closer to another GPU token (e.g., B200)\n",
    "        if competitor_positions:\n",
    "            nearest_other = min(abs(ppos - op) for op in competitor_positions)\n",
    "            if nearest_other < dist:\n",
    "                continue  # skip: price belongs to another GPU mention\n",
    "\n",
    "        rows.append((token, price))\n",
    "    return rows\n",
    "\n",
    "# --------- scraper ----------\n",
    "async def scrape_shadeform_rich() -> pd.DataFrame:\n",
    "    url = \"https://www.shadeform.ai/\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_price_to_token(body, gpu, window=220):\n",
    "            # sanity clamp to avoid accidental captures (tune if needed)\n",
    "            if not (0.25 <= price <= 8.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"Shadeform\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# Example:\n",
    "df_shade = arun(scrape_shadeform_rich())\n",
    "display(df_shade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paperspace] snapshot -> docs/data/snapshots/20250910_122224_paperspace.csv\n",
      "[paperspace] history  -> docs/data/history/paperspace_history.csv\n",
      "[paperspace] latest   -> docs/data/latest/paperspace_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.24</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "      <td>2025-09-10 12:22:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Paperspace  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                          source_url      fetched_at_utc  \n",
       "0              2.24  https://www.paperspace.com/pricing 2025-09-10 12:22:24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Paperspace (async) — use your approach, keep correct rows =================\n",
    "# Output schema (slim): provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                       price_hourly_usd, source_url, fetched_at_utc\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\"); HIST_DIR = BASE/\"history\"; SNAP_DIR = BASE/\"snapshots\"; LATEST_DIR = BASE/\"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df); ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR/f\"{ts}_{slug}.csv\")\n",
    "    # history\n",
    "    hist = HIST_DIR/f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "                    .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                             \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "                    .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR/f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- strict extractors (but still tolerant to site markup) ----\n",
    "GPU_PAT = re.compile(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", re.I)\n",
    "# require an hourly hint somewhere in the same block to avoid platform prices, etc.\n",
    "PRICE_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "async def scrape_paperspace() -> pd.DataFrame:\n",
    "    url = \"https://www.paperspace.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000)               # your simple navigation\n",
    "        await page.wait_for_timeout(8000)                  # your “just wait a few seconds”\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    rows = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # scan reasonable blocks; stick to your block-scan approach\n",
    "    for blk in soup.find_all([\"tr\",\"div\",\"section\",\"article\",\"li\"], recursive=True):\n",
    "        txt = blk.get_text(\" \", strip=True)\n",
    "        if not txt: \n",
    "            continue\n",
    "        # must mention H100/H200 AND 'hour' to qualify\n",
    "        if (\"H100\" not in txt and \"H200\" not in txt) or (\"hour\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        # model: first explicit H100/H200-ish token found\n",
    "        mm = GPU_PAT.search(txt)\n",
    "        if not mm:\n",
    "            continue\n",
    "        model = mm.group(1).upper()\n",
    "\n",
    "        # price: $… with an hourly hint in the same block\n",
    "        pm = PRICE_HOURLY.search(txt)\n",
    "        if not pm:\n",
    "            continue\n",
    "        price = float(pm.group(1))\n",
    "        # sanity band to drop weird captures\n",
    "        if not (0.2 <= price <= 50.0):\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"provider\": \"Paperspace\",\n",
    "            \"region\": \"Global\",\n",
    "            \"gpu_model\": model,          # \"H100\", \"H100 PCIE 80GB\", etc.\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": None,\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in both scripts & notebooks ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_paperspace = arun(scrape_paperspace())\n",
    "latest_paperspace = _save_provider(df_paperspace, \"paperspace\")\n",
    "display(df_paperspace.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensordock] snapshot -> docs/data/snapshots/20250910_122225_tensordock.csv\n",
      "[tensordock] history  -> docs/data/history/tensordock_history.csv\n",
      "[tensordock] latest   -> docs/data/latest/tensordock_latest.csv\n",
      "     provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  TensorDock  Global      H100  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                       source_url      fetched_at_utc  \n",
      "0              2.25  https://tensordock.com/gpu-h100 2025-09-10 12:22:24  \n"
     ]
    }
   ],
   "source": [
    "# ================= TensorDock H100 (static) — slim schema + per-provider history =================\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, requests, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- pages & patterns (from your code) --------\n",
    "PAGES = [\n",
    "    \"https://tensordock.com/gpu-h100\",\n",
    "    \"https://tensordock.com/cloud-gpus\",\n",
    "    \"https://tensordock.com/comparison-gcp\",\n",
    "]\n",
    "PATTERNS = [\n",
    "    re.compile(r\"H100.*?\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr\", re.I|re.S),\n",
    "    re.compile(r\"from\\s*\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr.*?H100\", re.I|re.S),\n",
    "    re.compile(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hour.*?H100\", re.I|re.S),\n",
    "]\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fall back handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- scraper (uses your logic, mapped to slim schema) --------\n",
    "def scrape_tensordock_public_h100() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for url in PAGES:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "            price = None\n",
    "            for pat in PATTERNS:\n",
    "                m = pat.search(text)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    break\n",
    "            if price and (0.2 <= price <= 50.0):  # sanity band for $/GPU/hr\n",
    "                rows.append({\n",
    "                    \"provider\": \"TensorDock\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": \"H100\",\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": 1,\n",
    "                    \"price_hourly_usd\": price,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[TensorDock] {url} -> {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        return _ensure_slim(pd.DataFrame(columns=SLIM_COLS))\n",
    "\n",
    "    # Deduplicate: keep the **lowest** \"from\" price across pages\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = (df.sort_values(\"price_hourly_usd\")\n",
    "            .drop_duplicates(subset=[\"provider\",\"gpu_model\"], keep=\"first\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_tensordock = scrape_tensordock_public_h100()\n",
    "latest_tensordock = _save_provider(df_tensordock, \"tensordock\")\n",
    "print(df_tensordock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[brokkr] snapshot -> docs/data/snapshots/20250910_122229_brokkr.csv\n",
      "[brokkr] history  -> docs/data/history/brokkr_history.csv\n",
      "[brokkr] latest   -> docs/data/latest/brokkr_latest.csv\n",
      "              provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
      "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                              source_url  \\\n",
      "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
      "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
      "\n",
      "       fetched_at_utc  \n",
      "0 2025-09-10 12:22:28  \n",
      "1 2025-09-10 12:22:28  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-10 12:22:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-10 12:22:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
       "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                              source_url  \\\n",
       "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
       "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-10 12:22:28  \n",
       "1 2025-09-10 12:22:28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== Hydra Host (Brokkr) — slim schema + per-provider history ==============\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same pattern as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- your Brokkr scraper, tightened to only accept \"per card-hour\" prices --------\n",
    "GPU_RE = r\"(H100|H200)\"\n",
    "PRICE_PER_CARDHR = r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:per\\s*card[-\\s]?hour|/card[-\\s]?hour)\\b\"\n",
    "PATS = [\n",
    "    re.compile(rf\"{GPU_RE}.{{0,220}}?{PRICE_PER_CARDHR}\", re.I | re.S),\n",
    "    re.compile(rf\"{PRICE_PER_CARDHR}.{{0,220}}?{GPU_RE}\", re.I | re.S),\n",
    "]\n",
    "\n",
    "async def scrape_brokkr() -> pd.DataFrame:\n",
    "    url = \"https://brokkr.hydrahost.com/inventory\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help hydrate lazy content\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(700)\n",
    "        body_text = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", body_text)\n",
    "    rows = []\n",
    "\n",
    "    for pat in PATS:\n",
    "        for m in pat.finditer(text):\n",
    "            # Depending on which pattern matched, group order differs\n",
    "            groups = m.groups()\n",
    "            # Normalize extraction: model + price are always present\n",
    "            if len(groups) == 2:\n",
    "                # pattern 1: (GPU, price)\n",
    "                gpu_model, price_str = groups\n",
    "            elif len(groups) == 3:\n",
    "                # pattern 2 returns (price, GPU) because of nested groups; pick numeric+gpu\n",
    "                # groups could be ('12.34', 'H100') or ('12.34', 'card-hour', 'H100') depending on regex engine\n",
    "                nums = [g for g in groups if g and re.fullmatch(r\"[0-9]+(?:\\.[0-9]+)?\", g)]\n",
    "                gpus = [g for g in groups if g and re.fullmatch(r\"H100|H200\", g, flags=re.I)]\n",
    "                if not nums or not gpus:\n",
    "                    continue\n",
    "                price_str, gpu_model = nums[0], gpus[0]\n",
    "            else:\n",
    "                # Safe fallback: find first number and first GPU token in the match\n",
    "                seg = m.group(0)\n",
    "                pm = re.search(r\"[0-9]+(?:\\.[0-9]+)?\", seg)\n",
    "                gm = re.search(r\"H100|H200\", seg, flags=re.I)\n",
    "                if not (pm and gm):\n",
    "                    continue\n",
    "                price_str, gpu_model = pm.group(0), gm.group(0)\n",
    "\n",
    "            try:\n",
    "                price = float(price_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # sanity band for per-card hour pricing\n",
    "            if not (0.2 <= price <= 50.0):\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"provider\": \"Hydra Host (Brokkr)\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": gpu_model.upper(),\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": price,   # per card-hour = per-GPU hourly\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner that works in both notebooks & scripts --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_brokkr = arun(scrape_brokkr())\n",
    "latest_brokkr = _save_provider(df_brokkr, \"brokkr\")\n",
    "print(df_brokkr)\n",
    "display(df_brokkr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crusoecloud] snapshot -> docs/data/snapshots/20250910_122232_crusoecloud.csv\n",
      "[crusoecloud] history  -> docs/data/history/crusoecloud_history.csv\n",
      "[crusoecloud] latest   -> docs/data/latest/crusoecloud_latest.csv\n",
      "      provider  region gpu_model         type duration  gpu_count  \\\n",
      "0  CrusoeCloud  Global      H100    On-Demand       1h          1   \n",
      "1  CrusoeCloud  Global      H100  Reserved-1y       1h          1   \n",
      "2  CrusoeCloud  Global      H100  Reserved-3y       1h          1   \n",
      "3  CrusoeCloud  Global      H100  Reserved-6m       1h          1   \n",
      "4  CrusoeCloud  Global      H200    On-Demand       1h          1   \n",
      "5  CrusoeCloud  Global      H200  Reserved-1y       1h          1   \n",
      "6  CrusoeCloud  Global      H200  Reserved-3y       1h          1   \n",
      "7  CrusoeCloud  Global      H200  Reserved-6m       1h          1   \n",
      "\n",
      "   price_hourly_usd                           source_url      fetched_at_utc  \n",
      "0              3.90  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "1              2.93  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "2              2.54  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "3              3.12  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "4              4.29  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "5              3.22  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "6              2.79  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n",
      "7              3.43  https://www.crusoe.ai/cloud/pricing 2025-09-10 12:22:32  \n"
     ]
    }
   ],
   "source": [
    "# ============ Crusoe Cloud (async) — table scrape → slim schema + history ============\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------- slim schema + storage helpers (same as other providers) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{slug}.csv\")\n",
    "    # history append+dedupe\n",
    "    hist = HIST_DIR / f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "        .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "        .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                 \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "        .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest per gpu/type/region/duration\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- parsing helpers ----------\n",
    "PRICE_RE = re.compile(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "def _parse_price(cell_text: str):\n",
    "    if not cell_text: return None\n",
    "    m = PRICE_RE.search(cell_text.replace(\",\", \"\"))\n",
    "    if not m:\n",
    "        # fallback: plain $N.NN without explicit /hr\n",
    "        m2 = re.search(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\b\", cell_text.replace(\",\", \"\"))\n",
    "        return float(m2.group(1)) if m2 else None\n",
    "    return float(m.group(1))\n",
    "\n",
    "def _is_h_model(text: str) -> bool:\n",
    "    t = text.upper()\n",
    "    return (\"H100\" in t) or (\"H200\" in t)\n",
    "\n",
    "def _model_from(text: str) -> str:\n",
    "    return \"H100\" if \"H100\" in text.upper() else \"H200\"\n",
    "\n",
    "# ---------- scraper ----------\n",
    "async def scrape_crusoe_table() -> pd.DataFrame:\n",
    "    url = \"https://www.crusoe.ai/cloud/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help render\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows_out = []\n",
    "\n",
    "    # Find all table rows; filter to those that mention H100/H200\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        tds = [c.get_text(\" \", strip=True) for c in tr.find_all(\"td\")]\n",
    "        if not tds or not any(_is_h_model(c) for c in tds):\n",
    "            continue\n",
    "\n",
    "        model = _model_from(\" \".join(tds))\n",
    "\n",
    "        # Try to map columns conservatively:\n",
    "        # Common layout: [Model, On-Demand, Spot?, Reserved 6m, Reserved 1y, Reserved 3y, ...]\n",
    "        # We’ll grab by position if present, else try to read by header alignment.\n",
    "        on_demand = _parse_price(tds[1]) if len(tds) > 1 else None\n",
    "        res_6m    = _parse_price(tds[3]) if len(tds) > 3 else None\n",
    "        res_1y    = _parse_price(tds[4]) if len(tds) > 4 else None\n",
    "        res_3y    = _parse_price(tds[5]) if len(tds) > 5 else None\n",
    "\n",
    "        # Build normalised slim rows\n",
    "        if on_demand is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": on_demand,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_6m is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-6m\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_6m,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_1y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-1y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_1y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_3y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-3y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_3y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe & sanity\n",
    "    df = (df.sort_values([\"gpu_model\",\"type\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"type\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # plausible hourly band\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---------- runner that works in scripts & notebooks ----------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_crusoe = arun(scrape_crusoe_table())\n",
    "latest_crusoe = _save_provider(df_crusoe, \"crusoecloud\")\n",
    "print(df_crusoe.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   provider  region gpu_model       instance_type  gpu_count  \\\n",
      "0  OVHcloud  Global      H100  public-cloud/Price          2   \n",
      "1  OVHcloud  Global      H100  public-cloud/Price          4   \n",
      "\n",
      "   price_hourly_usd_instance  price_hourly_usd_per_gpu price_reserved_usd  \\\n",
      "0                       5.98                    2.9900               None   \n",
      "1                      11.97                    2.9925               None   \n",
      "\n",
      "  reserved_duration                   timestamp  \n",
      "0              None  2025-09-10T12:22:33.642693  \n",
      "1              None  2025-09-10T12:22:33.642767  \n"
     ]
    }
   ],
   "source": [
    "# OVHcloud H100/H200 — get the *correct per-GPU hourly price* from the public prices table\n",
    "# - Ties each $…/hour to the same row as H100/H200\n",
    "# - Extracts the GPU count from the row (1×/2×/4×/8× or “… GPUs”)\n",
    "# - per_gpu = instance_price / parsed_gpu_count  (NO 8× assumption)\n",
    "# - Returns both instance price and per-GPU price\n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def timestamp(): return datetime.utcnow().isoformat()\n",
    "\n",
    "USD_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "# GPU count detectors (row-level + cell/variant-level)\n",
    "COUNT_PATS = [\n",
    "    re.compile(r\"(\\d+)\\s*[×x]\\s*(?:NVIDIA\\s*)?(H100|H200)\\b\", re.I),  # \"8× H100\"\n",
    "    re.compile(r\"(H100|H200)\\s*[×x]\\s*(\\d+)\\b\", re.I),               # \"H100 × 8\"\n",
    "    re.compile(r\"(\\d+)\\s*(?:GPU|GPUs)\\b\", re.I),                     # \"8 GPUs\"\n",
    "    re.compile(r\"\\b(\\d+)\\s*[×x]\\b\", re.I),                           # \"4x\"\n",
    "]\n",
    "\n",
    "def _parse_count(text: str):\n",
    "    for pat in COUNT_PATS:\n",
    "        m = pat.search(text)\n",
    "        if not m: \n",
    "            continue\n",
    "        for g in m.groups():\n",
    "            if g and g.isdigit():\n",
    "                n = int(g)\n",
    "                if 1 <= n <= 16:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def scrape_ovhcloud_correct(url=\"https://www.ovhcloud.com/en/public-cloud/prices/\") -> pd.DataFrame:\n",
    "    html = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=60).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for table in soup.select(\"table\"):\n",
    "        # headers so we can label which column the number came from\n",
    "        headers = [th.get_text(\" \", strip=True) for th in table.select(\"thead th\")]\n",
    "        if not headers:\n",
    "            first = table.find(\"tr\")\n",
    "            if first:\n",
    "                headers = [td.get_text(\" \", strip=True) for td in first.find_all([\"th\",\"td\"])]\n",
    "\n",
    "        body_rows = table.select(\"tbody tr\") or table.select(\"tr\")\n",
    "        for tr in body_rows:\n",
    "            tds = tr.find_all(\"td\")\n",
    "            if not tds: \n",
    "                continue\n",
    "            cells = [td.get_text(\" \", strip=True) for td in tds]\n",
    "            row_txt = \" \".join(cells)\n",
    "            up = row_txt.upper()\n",
    "            if (\"H100\" not in up) and (\"H200\" not in up):\n",
    "                continue\n",
    "\n",
    "            model = \"H100\" if \"H100\" in up else \"H200\"\n",
    "            row_count = _parse_count(row_txt)\n",
    "\n",
    "            for idx, (td, cell) in enumerate(zip(tds, cells)):\n",
    "                m = USD_HOURLY.search(cell)\n",
    "                if not m:\n",
    "                    continue\n",
    "                instance_price = float(m.group(1))\n",
    "\n",
    "                # try counts in cell and header/variant too\n",
    "                var = headers[idx] if idx < len(headers) and headers else f\"col_{idx+1}\"\n",
    "                count = (\n",
    "                    _parse_count(cell) or\n",
    "                    _parse_count(var)  or\n",
    "                    row_count\n",
    "                )\n",
    "                if count is None:\n",
    "                    # if we can't prove node size, skip (prevents wrong divide)\n",
    "                    continue\n",
    "\n",
    "                per_gpu = instance_price / count\n",
    "                if not (0.25 <= per_gpu <= 20.0):\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"OVHcloud\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": model,\n",
    "                    \"instance_type\": f\"public-cloud/{var}\",\n",
    "                    \"gpu_count\": int(count),\n",
    "                    \"price_hourly_usd_instance\": round(instance_price, 4),\n",
    "                    \"price_hourly_usd_per_gpu\": round(per_gpu, 4),\n",
    "                    \"price_reserved_usd\": None,\n",
    "                    \"reserved_duration\": None,\n",
    "                    \"timestamp\": timestamp(),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = (df.sort_values([\"gpu_model\",\"price_hourly_usd_instance\"])\n",
    "                .drop_duplicates(subset=[\"gpu_model\",\"instance_type\",\"price_hourly_usd_instance\"], keep=\"last\")\n",
    "                .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# Example\n",
    "df_ovh = scrape_ovhcloud_correct()\n",
    "print(df_ovh.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote docs/data/derived/provider_scores_latest.csv (24 rows)\n",
      "✔ wrote docs/data/derived/roi_comparison.csv (32 rows)\n",
      "✔ wrote docs/data/derived/price_predict_snapshot.json\n",
      "{\n",
      "  \"asof_utc\": \"2025-09-10T15:28:44+00:00\",\n",
      "  \"gpu_model\": \"H100\",\n",
      "  \"type\": \"On-Demand\",\n",
      "  \"gpu_vram_gb\": null,\n",
      "  \"cpu_cores\": null,\n",
      "  \"cpu_ram_gb\": null,\n",
      "  \"region\": \"US\",\n",
      "  \"n\": 1,\n",
      "  \"used_region\": \"US\",\n",
      "  \"p10\": 1.99,\n",
      "  \"p25\": 1.99,\n",
      "  \"p50\": 1.99,\n",
      "  \"p75\": 1.99,\n",
      "  \"p90\": 1.99\n",
      "}\n",
      "✔ wrote docs/data/derived/market_index.csv\n",
      "\n",
      "Preview — provider_scores_latest.csv:\n",
      "    provider  region gpu_model       type  effective_price_usd_per_gpu_hr  market_median  priceiq_score\n",
      "    OVHcloud  Global      H100  On-Demand                          2.9900           2.25      31.297884\n",
      "    OVHcloud  Global      H100  On-Demand                          2.9925           2.25      26.428724\n",
      "      Nebius      US      H200  On-Demand                          2.3000           2.30      11.873683\n",
      "   Shadeform  Global      H200  On-Demand                          2.4500           2.30      74.373683\n",
      "   SFCompute      US      H200  On-Demand                          1.3800           2.30      86.873683\n",
      "   SFCompute      US      H200  On-Demand                          1.4000           2.30      49.373683\n",
      "      Nebius  Global      H200  On-Demand                          2.3000           2.30      86.873683\n",
      "      Nebius  Global      H100  On-Demand                          2.0000           2.25      74.373683\n",
      " VoltagePark      US      H100  On-Demand                          1.9900           2.25      86.873683\n",
      "     Vast.ai  Global      H100  On-Demand                          1.2500           2.25      86.873683\n",
      "\n",
      "Preview — roi_comparison.csv:\n",
      "gpu_model  gpu_count duration best_provider best_region  price_per_gpu_hr  total_cost_usd  timestamp\n",
      "     H100          8   1 hour       Vast.ai      Global              1.25            10.0        NaN\n",
      "     H100          8    1 day       Vast.ai      Global              1.25           240.0        NaN\n",
      "     H100          8   1 week       Vast.ai      Global              1.25          1680.0        NaN\n",
      "     H100          8  1 month       Vast.ai      Global              1.25          7200.0        NaN\n",
      "     H100         16   1 hour       Vast.ai      Global              1.25            20.0        NaN\n",
      "     H100         16    1 day       Vast.ai      Global              1.25           480.0        NaN\n"
     ]
    }
   ],
   "source": [
    "# === ONE-CELL: unify → baselines (On-Demand) → score → ROI → Silicon-style prediction ===\n",
    "# Bullet-proof: guarantees per-GPU $/hr column and avoids KeyErrors. Py3.8 friendly.\n",
    "\n",
    "import re, math, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ---------- config ----------\n",
    "BASE = Path(\"docs/data\")\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "DERIVED_DIR = BASE / \"derived\"\n",
    "for d in (LATEST_DIR, DERIVED_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "PRICE_COL = \"effective_price_usd_per_gpu_hr\"  # single source of truth\n",
    "\n",
    "# providers to exclude from *all* outputs\n",
    "PROVIDERS_EXCLUDE = [\"CoreWeave\"]\n",
    "_excl_set = {p.casefold() for p in PROVIDERS_EXCLUDE}\n",
    "\n",
    "def _exclude_providers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"provider\" not in df.columns:\n",
    "        return df\n",
    "    prov_norm = df[\"provider\"].astype(str).str.strip().str.casefold()\n",
    "    return df[~prov_norm.isin(_excl_set)].copy()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def to_hours(s):\n",
    "    if not isinstance(s,str) or not s.strip(): return np.nan\n",
    "    s = s.strip().lower()\n",
    "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)\\s*([a-z]+)s?$\", s)\n",
    "    if not m: return np.nan\n",
    "    q, u = float(m.group(1)), m.group(2)\n",
    "    mult = {\"h\":1,\"hr\":1,\"hour\":1,\"d\":24,\"day\":24,\"w\":168,\"wk\":168,\"week\":168,\"mo\":720,\"month\":720}.get(u, np.nan)\n",
    "    return q*mult\n",
    "\n",
    "def _numify(series):\n",
    "    # handles None/'None'/'$1,234'/ '1.2/hr' etc.\n",
    "    return pd.to_numeric(series.astype(str).str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True), errors=\"coerce\")\n",
    "\n",
    "EXPECTED = {\n",
    "    \"provider\": [\"provider\",\"Provider\",\"name\"],\n",
    "    \"region\": [\"region\",\"Region\",\"location\",\"geo\",\"cloud_region\",\"area\"],\n",
    "    \"gpu_model\": [\"gpu_model\",\"gpu\",\"model\",\"GPU\"],\n",
    "    \"gpu_count\": [\"gpu_count\",\"count\",\"gpus\",\"GPU_count\"],\n",
    "    \"type\": [\"type\",\"price_type\"],\n",
    "    \"duration\": [\"duration\",\"reserved_duration\",\"term\"],\n",
    "    \"price_hourly_usd\": [\"price_hourly_usd\",\"price\",\"usd_per_gpu_hr\",\"price_hourly_usd_per_gpu\"],\n",
    "    \"price_reserved_usd\": [\"price_reserved_usd\",\"reserved_usd_per_gpu_hr\",\"reserved_price\"],\n",
    "    \"price_hourly_usd_instance\": [\"price_hourly_usd_instance\",\"instance_price\",\"node_hourly_usd\"],\n",
    "    \"timestamp\": [\"timestamp\",\"fetched_at_utc\",\"ts_utc\",\"ts_iso\",\"time\",\"start_time_iso\"],\n",
    "    \"source_url\": [\"source_url\",\"url\",\"link\"],\n",
    "    # optional specs (kept, but NOT used in prediction right now)\n",
    "    \"gpu_vram_gb\": [\"gpu_vram_gb\",\"gpu_ram_gb\",\"vram_gb\"],\n",
    "    \"cpu_cores\": [\"cpu_cores\",\"vcpus\",\"cpu\",\"cpu_count\"],\n",
    "    \"cpu_ram_gb\": [\"cpu_ram_gb\",\"memory_gb\",\"mem_gb\",\"ram_gb\"],\n",
    "}\n",
    "\n",
    "def _map_expected(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for tgt, cands in EXPECTED.items():\n",
    "        if tgt not in out.columns:\n",
    "            for c in cands:\n",
    "                if c in out.columns:\n",
    "                    out[tgt] = out[c]; break\n",
    "        if tgt not in out.columns:\n",
    "            out[tgt] = pd.NA\n",
    "    return out\n",
    "\n",
    "def _derive_effective_price(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Always returns float Series for per-GPU $/hr, derived from any price fields + duration.\"\"\"\n",
    "    n = len(df)\n",
    "    eff = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "    # direct per-GPU if present\n",
    "    if PRICE_COL in df.columns:\n",
    "        eff = pd.to_numeric(df[PRICE_COL], errors=\"coerce\")\n",
    "    # explicit per-GPU hourly\n",
    "    if \"price_hourly_usd\" in df.columns:\n",
    "        ph = _numify(df[\"price_hourly_usd\"])\n",
    "        eff = eff.fillna(ph)\n",
    "    # split instance hourly by gpu_count\n",
    "    if \"price_hourly_usd_instance\" in df.columns:\n",
    "        inst = _numify(df[\"price_hourly_usd_instance\"])\n",
    "        g = pd.to_numeric(df.get(\"gpu_count\", pd.Series([np.nan]*n)), errors=\"coerce\")\n",
    "        m = eff.isna() & inst.notna() & g.notna() & (g > 0)\n",
    "        eff.loc[m] = inst.loc[m] / g.loc[m]\n",
    "    # reserved: detect total-for-term vs hourly\n",
    "    if \"price_reserved_usd\" in df.columns:\n",
    "        res = _numify(df[\"price_reserved_usd\"])\n",
    "        dur_h = df.get(\"duration\", pd.Series([\"\"]*n)).apply(to_hours)\n",
    "        m_tot = eff.isna() & res.notna() & dur_h.notna() & res.gt(10)\n",
    "        eff.loc[m_tot] = res.loc[m_tot] / dur_h.loc[m_tot]\n",
    "        m_hr = eff.isna() & res.notna() & ~m_tot\n",
    "        eff.loc[m_hr] = res.loc[m_hr]\n",
    "    return pd.to_numeric(eff, errors=\"coerce\")\n",
    "\n",
    "def normalise_any(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Canonicalize cols + compute per-GPU $/hr. Keeps only sane H100/H200 rows.\"\"\"\n",
    "    df = _map_expected(df)\n",
    "    # types & defaults\n",
    "    df[\"gpu_model\"] = df[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "    df[\"region\"] = df[\"region\"].astype(str).replace({\"nan\":None}).fillna(\"Global\").str.strip()\n",
    "    df[\"type\"] = df[\"type\"].astype(str).replace({\"nan\":None}).fillna(\"On-Demand\").str.strip()\n",
    "    df[\"duration\"] = df[\"duration\"].astype(str).replace({\"nan\":None,\"\":None}).fillna(\"1h\").str.strip()\n",
    "    df[\"gpu_count\"] = pd.to_numeric(df[\"gpu_count\"], errors=\"coerce\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "    # derive per-GPU $/hr\n",
    "    df[PRICE_COL] = _derive_effective_price(df)\n",
    "    # bounds + models\n",
    "    df = df[df[\"gpu_model\"].isin([\"H100\",\"H200\"])]\n",
    "    df = df[df[PRICE_COL].between(0.05, 200)].copy()\n",
    "    # final ISO timestamp\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    keep = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "            \"price_hourly_usd\",\"price_reserved_usd\",\"price_hourly_usd_instance\",\n",
    "            PRICE_COL,\"timestamp\",\"source_url\",\"gpu_vram_gb\",\"cpu_cores\",\"cpu_ram_gb\"]\n",
    "    extra = [c for c in df.columns if c not in keep]\n",
    "    return df[keep + extra]\n",
    "\n",
    "def ensure_price_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Guarantee a numeric per-GPU $/hr column named PRICE_COL.\"\"\"\n",
    "    out = df.copy()\n",
    "    # exact column\n",
    "    if PRICE_COL in out.columns:\n",
    "        out[PRICE_COL] = pd.to_numeric(out[PRICE_COL], errors=\"coerce\")\n",
    "    # similar-name rescue\n",
    "    if PRICE_COL not in out.columns or out[PRICE_COL].isna().all():\n",
    "        cand = None\n",
    "        for c in out.columns:\n",
    "            if re.search(r\"effective.*gpu.*(hr|hour)\", c, flags=re.I):\n",
    "                cand = c; break\n",
    "        if cand and cand != PRICE_COL:\n",
    "            out = out.rename(columns={cand: PRICE_COL})\n",
    "            out[PRICE_COL] = pd.to_numeric(out[PRICE_COL], errors=\"coerce\")\n",
    "    # last resort: derive\n",
    "    if PRICE_COL not in out.columns or out[PRICE_COL].isna().all():\n",
    "        out[PRICE_COL] = _derive_effective_price(out)\n",
    "    out[PRICE_COL] = pd.to_numeric(out[PRICE_COL], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def percentile_rev(s: pd.Series) -> pd.Series:\n",
    "    r = s.rank(pct=True, method=\"average\")\n",
    "    return 1.0 - (r - r.min())/(r.max()-r.min()+1e-12)\n",
    "\n",
    "def add_priceiq_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    grp = [\"gpu_model\",\"region\"] if df[\"region\"].nunique() > 1 else [\"gpu_model\"]\n",
    "    df[\"_pct\"] = df.groupby(grp)[PRICE_COL].transform(percentile_rev)\n",
    "    cap = np.log1p(df[\"gpu_count\"].fillna(1))/np.log1p(256)  # 0..1\n",
    "    df[\"_cap\"] = 15.0*cap.clip(0,1)  # up to +15\n",
    "    def rec(ts):\n",
    "        try:\n",
    "            t = pd.to_datetime(ts, utc=True)\n",
    "            age_h = max(0.0, (pd.Timestamp.utcnow()-t).total_seconds()/3600)\n",
    "            return 10.0*math.exp(-age_h/72.0)  # up to +10\n",
    "        except:\n",
    "            return 0.0\n",
    "    df[\"_rec\"] = df[\"timestamp\"].map(rec)\n",
    "    df[\"priceiq_score\"] = (df[\"_pct\"]*75.0 + df[\"_cap\"] + df[\"_rec\"]).clip(0,100)\n",
    "    return df.drop(columns=[\"_pct\",\"_cap\",\"_rec\"], errors=\"ignore\")\n",
    "\n",
    "# ---------- gather frames ----------\n",
    "frames = []\n",
    "\n",
    "# iterate over a *dict* (copy) so .items() exists; safe if globals change during loop\n",
    "for name, obj in globals().copy().items():\n",
    "    if isinstance(obj, pd.DataFrame) and (name.startswith((\"df_\",\"latest_\"))) and not obj.empty:\n",
    "        frames.append(obj.copy())\n",
    "\n",
    "if not frames:\n",
    "    for p in sorted(LATEST_DIR.glob(\"*_latest.csv\")):\n",
    "        try:\n",
    "            frames.append(pd.read_csv(p, low_memory=False))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to load {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No provider frames found (in-memory or docs/data/latest/*). Run scrapers first.\")\n",
    "\n",
    "\n",
    "# -------- EXCLUSION: drop CoreWeave from everything --------\n",
    "norm = _exclude_providers(norm)\n",
    "\n",
    "norm = ensure_price_col(norm)\n",
    "norm = norm[norm[PRICE_COL].between(0.05, 200)]\n",
    "if norm.empty:\n",
    "    raise RuntimeError(\"No usable per-GPU prices after normalization.\")\n",
    "\n",
    "# de-dupe newest by key\n",
    "key = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",PRICE_COL]\n",
    "norm = (norm.sort_values(\"timestamp\")\n",
    "            .drop_duplicates(subset=key, keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# ---------- 1) Market baselines (On-Demand only; flat columns) ----------\n",
    "gcols = [\"gpu_model\", \"region\"]  # pool for baselines\n",
    "od = norm[norm[\"type\"].str.contains(\"On-Demand\", case=False, na=False) & norm[PRICE_COL].notna()].copy()\n",
    "if od.empty:\n",
    "    raise RuntimeError(\"No On-Demand rows available to build market baselines.\")\n",
    "\n",
    "# build market stats with named aggregations (flat columns in all pandas versions)\n",
    "market = (\n",
    "    od.groupby(gcols, as_index=False)\n",
    "      .agg(\n",
    "          market_count =(PRICE_COL, 'size'),\n",
    "          market_mean  =(PRICE_COL, 'mean'),\n",
    "          market_median=(PRICE_COL, 'median'),\n",
    "          market_p10   =(PRICE_COL, lambda s: s.quantile(0.10)),\n",
    "          market_p25   =(PRICE_COL, lambda s: s.quantile(0.25)),\n",
    "          market_p75   =(PRICE_COL, lambda s: s.quantile(0.75)),\n",
    "          market_p90   =(PRICE_COL, lambda s: s.quantile(0.90)),\n",
    "      )\n",
    ")\n",
    "\n",
    "# merge onto the full set\n",
    "scored = norm.merge(market, on=gcols, how=\"left\").copy()\n",
    "\n",
    "# NEW GUARD: ensure market_* columns exist even if merge produced none (edge cases)\n",
    "for _c in [\"market_count\",\"market_mean\",\"market_median\",\"market_p25\",\"market_p75\",\"market_p10\",\"market_p90\"]:\n",
    "    if _c not in scored.columns:\n",
    "        scored[_c] = np.nan\n",
    "\n",
    "# if any older pandas produced MultiIndex columns for any reason, flatten as a safeguard\n",
    "if any(isinstance(c, tuple) for c in scored.columns):\n",
    "    scored.columns = [\"_\".join(str(x) for x in c if x) if isinstance(c, tuple) else c]\n",
    "\n",
    "# guarantee the price column exists post-merge\n",
    "scored = ensure_price_col(scored)\n",
    "\n",
    "# --- premium vs market median (robust Series, safe zero/NaN guard) ---\n",
    "if \"market_median\" in scored.columns:\n",
    "    mm = pd.to_numeric(scored[\"market_median\"], errors=\"coerce\")\n",
    "else:\n",
    "    mm = pd.Series(np.nan, index=scored.index, dtype=\"float64\")\n",
    "mm = mm.mask(mm == 0)  # treat zeros as missing to avoid div/0\n",
    "scored[\"premium_vs_median\"] = (scored[PRICE_COL] - mm) / mm\n",
    "scored[\"asof_utc\"] = _now_iso()\n",
    "\n",
    "# ---- fill any missing market stats with model-wide fallbacks, then self-price as last resort\n",
    "missing = scored[\"market_median\"].isna() if \"market_median\" in scored.columns else pd.Series(True, index=scored.index)\n",
    "if missing.any():\n",
    "    fb = (\n",
    "        od.groupby([\"gpu_model\"])[PRICE_COL]\n",
    "          .agg(\n",
    "              market_count_fb=\"size\",\n",
    "              market_mean_fb =\"mean\",\n",
    "              market_median_fb=\"median\",\n",
    "              market_p25_fb  =lambda s: s.quantile(0.25),\n",
    "              market_p75_fb  =lambda s: s.quantile(0.75),\n",
    "              market_p10_fb  =lambda s: s.quantile(0.10),\n",
    "              market_p90_fb  =lambda s: s.quantile(0.90),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "    scored = scored.merge(fb, on=\"gpu_model\", how=\"left\")\n",
    "    for col in [\"market_count\",\"market_mean\",\"market_median\",\"market_p25\",\"market_p75\",\"market_p10\",\"market_p90\"]:\n",
    "        if col in scored.columns and f\"{col}_fb\" in scored.columns:\n",
    "            scored[col] = scored[col].where(scored[col].notna(), scored[f\"{col}_fb\"])\n",
    "    scored.drop(columns=[c for c in scored.columns if c.endswith(\"_fb\")], inplace=True, errors=\"ignore\")\n",
    "    # last resort: if still NaN, copy row price into market columns\n",
    "    for col in [\"market_median\",\"market_mean\",\"market_p25\",\"market_p75\",\"market_p10\",\"market_p90\"]:\n",
    "        if col in scored.columns:\n",
    "            scored[col] = scored[col].fillna(scored[PRICE_COL])\n",
    "\n",
    "    # NEW SAFE RECOMPUTE: robust again even if column missing\n",
    "    mm = pd.to_numeric(scored.get(\"market_median\", pd.Series(np.nan, index=scored.index)), errors=\"coerce\")\n",
    "    mm = mm.mask(mm == 0)\n",
    "    scored[\"premium_vs_median\"] = (scored[PRICE_COL] - mm) / mm\n",
    "\n",
    "# ---------- 2) Price-IQ score & export ----------\n",
    "scored = add_priceiq_score(scored)\n",
    "scores_path = DERIVED_DIR / \"provider_scores_latest.csv\"\n",
    "scored.to_csv(scores_path, index=False)\n",
    "print(\"✔ wrote\", scores_path, f\"({len(scored)} rows)\")\n",
    "\n",
    "# ---------- 3) ROI (cheapest total cost by scenario, On-Demand pool) ----------\n",
    "def roi_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    base = df[df[PRICE_COL].notna()].copy()\n",
    "    if base.empty: return pd.DataFrame()\n",
    "    scenarios, models = [], [m for m in [\"H100\",\"H200\"] if m in set(base[\"gpu_model\"])] or sorted(base[\"gpu_model\"].unique())\n",
    "    for m in models:\n",
    "        for c in [8,16,32,64]:\n",
    "            for d in [\"1 hour\",\"1 day\",\"1 week\",\"1 month\"]:\n",
    "                h = to_hours(d)\n",
    "                if not h or np.isnan(h): continue\n",
    "                sub = base[base[\"gpu_model\"].eq(m)].copy()\n",
    "                if sub.empty: continue\n",
    "                sub[\"total_cost\"] = sub[PRICE_COL] * c * h\n",
    "                r = sub.nsmallest(1, [\"total_cost\", PRICE_COL])\n",
    "                if r.empty: continue\n",
    "                r = r.iloc[0]\n",
    "                scenarios.append({\n",
    "                    \"gpu_model\": m,\n",
    "                    \"gpu_count\": int(c),\n",
    "                    \"duration\": d,\n",
    "                    \"best_provider\": r[\"provider\"],\n",
    "                    \"best_region\": r[\"region\"],\n",
    "                    \"price_per_gpu_hr\": round(float(r[PRICE_COL]), 4),\n",
    "                    \"total_cost_usd\": round(float(r[\"total_cost\"]), 2),\n",
    "                    \"timestamp\": r[\"timestamp\"]\n",
    "                })\n",
    "    return pd.DataFrame(scenarios)\n",
    "\n",
    "roi_df = roi_table(od)\n",
    "roi_path = DERIVED_DIR / \"roi_comparison.csv\"\n",
    "roi_df.to_csv(roi_path, index=False)\n",
    "print(\"✔ wrote\", roi_path, f\"({len(roi_df)} rows)\")\n",
    "\n",
    "# ---------- 4) Silicon-style prediction (GPU + type + region; CPU/RAM = N/A for now) ----------\n",
    "REQ = dict(\n",
    "    gpu_model=\"H100\",          # \"H100\" or \"H200\"\n",
    "    type=\"On-Demand\",          # predict from On-Demand pool\n",
    "    gpu_vram_gb=None,          # N/A\n",
    "    cpu_cores=None,            # N/A\n",
    "    cpu_ram_gb=None,           # N/A\n",
    "    region=\"US\"                # prefer exact region; fallback Global → mixed\n",
    ")\n",
    "\n",
    "def predict_price_quantiles(data: pd.DataFrame, req: dict):\n",
    "    df = data[(data[\"gpu_model\"].str.upper()==str(req.get(\"gpu_model\",\"\")).upper())].copy()\n",
    "    if \"type\" in req and isinstance(req[\"type\"], str) and req[\"type\"].strip():\n",
    "        df = df[df[\"type\"].str.contains(req[\"type\"], case=False, na=False)]\n",
    "    if df.empty:\n",
    "        return {\"n\":0, \"used_region\":\"none\", \"p10\":None,\"p25\":None,\"p50\":None,\"p75\":None,\"p90\":None}\n",
    "    region = str(req.get(\"region\",\"Global\"))\n",
    "    df_reg = df[df[\"region\"]==region]\n",
    "    used_reg = region if not df_reg.empty else (\"Global\" if not df[df[\"region\"].str.lower().eq(\"global\")].empty else \"mixed\")\n",
    "    if df_reg.empty:\n",
    "        df_reg = df[df[\"region\"].str.lower().eq(\"global\")] if used_reg==\"Global\" else df\n",
    "    # equal weights; (optional) enable recency downweighting\n",
    "    # ages_h = (pd.Timestamp.utcnow() - pd.to_datetime(df_reg[\"timestamp\"], utc=True)).dt.total_seconds()/3600\n",
    "    # w = np.exp(-np.clip(ages_h,0,None)/72.0)\n",
    "    w = np.ones(len(df_reg), dtype=float)\n",
    "    vals = df_reg[PRICE_COL].astype(float).values\n",
    "    order = np.argsort(vals); v, ww = vals[order], w[order]\n",
    "    c = np.cumsum(ww); total = c[-1]\n",
    "    def q(p): return None if total<=0 else round(float(np.interp(p*total, c, v)), 4)\n",
    "    return {\"n\": int(len(df_reg)), \"used_region\": used_reg,\n",
    "            \"p10\": q(0.10), \"p25\": q(0.25), \"p50\": q(0.50), \"p75\": q(0.75), \"p90\": q(0.90)}\n",
    "\n",
    "pred = predict_price_quantiles(od, REQ)\n",
    "pred_record = {\"asof_utc\": _now_iso(), **REQ, **pred}\n",
    "(DERIVED_DIR / \"price_predict_snapshot.json\").write_text(json.dumps(pred_record, indent=2))\n",
    "print(\"✔ wrote\", DERIVED_DIR / \"price_predict_snapshot.json\")\n",
    "print(json.dumps(pred_record, indent=2))\n",
    "\n",
    "# ---------- 5) Export market index ----------\n",
    "market_path = DERIVED_DIR / \"market_index.csv\"\n",
    "market.to_csv(market_path, index=False)\n",
    "print(\"✔ wrote\", market_path)\n",
    "\n",
    "# Keep notebook output light\n",
    "_preview_cols = [c for c in [\"provider\",\"region\",\"gpu_model\",\"type\",PRICE_COL,\"market_median\",\"priceiq_score\"] if c in scored.columns]\n",
    "print(\"\\nPreview — provider_scores_latest.csv:\")\n",
    "print(scored[_preview_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nPreview — roi_comparison.csv:\")\n",
    "print(roi_df.head(6).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote docs/data/derived/tenant_roi_quotes.csv (1 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>utilization_pct</th>\n",
       "      <th>price_per_gpu_hr</th>\n",
       "      <th>total_cost_usd</th>\n",
       "      <th>price_source</th>\n",
       "      <th>source_url</th>\n",
       "      <th>asof_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1 week</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2674.56</td>\n",
       "      <td>observed</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-10T15:28:55+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region gpu_model       type duration  gpu_count  \\\n",
       "0  VoltagePark     US      H100  On-Demand   1 week          8   \n",
       "\n",
       "   utilization_pct  price_per_gpu_hr  total_cost_usd price_source  \\\n",
       "0              100              1.99         2674.56     observed   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://dashboard.voltagepark.com/order/config...   \n",
       "\n",
       "                    asof_utc  \n",
       "0  2025-09-10T15:28:55+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote docs/data/derived/provider_roi_projection.csv (60 rows)\n",
      "✔ wrote docs/data/derived/provider_roi_summary.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>price_per_gpu_hr</th>\n",
       "      <th>util_pct</th>\n",
       "      <th>delivered_util_pct</th>\n",
       "      <th>revenue_usd</th>\n",
       "      <th>power_cost_usd</th>\n",
       "      <th>other_opex_usd</th>\n",
       "      <th>depreciation_usd</th>\n",
       "      <th>ebitda_usd</th>\n",
       "      <th>ebit_usd</th>\n",
       "      <th>tax_usd</th>\n",
       "      <th>net_income_usd</th>\n",
       "      <th>cashflow_usd</th>\n",
       "      <th>cum_cash_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>35.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>93744.00</td>\n",
       "      <td>1693.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>92050.56</td>\n",
       "      <td>25383.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25383.89</td>\n",
       "      <td>-2307949.44</td>\n",
       "      <td>-2307949.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.282165</td>\n",
       "      <td>37.94</td>\n",
       "      <td>75.88</td>\n",
       "      <td>99749.42</td>\n",
       "      <td>1835.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>97913.68</td>\n",
       "      <td>31247.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31247.01</td>\n",
       "      <td>97913.68</td>\n",
       "      <td>-2210035.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.240120</td>\n",
       "      <td>40.88</td>\n",
       "      <td>81.76</td>\n",
       "      <td>105501.75</td>\n",
       "      <td>1978.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>103523.70</td>\n",
       "      <td>36857.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36857.03</td>\n",
       "      <td>103523.70</td>\n",
       "      <td>-2106512.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.198849</td>\n",
       "      <td>43.82</td>\n",
       "      <td>87.65</td>\n",
       "      <td>111008.26</td>\n",
       "      <td>2120.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>108887.90</td>\n",
       "      <td>42221.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42221.23</td>\n",
       "      <td>108887.90</td>\n",
       "      <td>-1997624.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.158339</td>\n",
       "      <td>46.76</td>\n",
       "      <td>93.53</td>\n",
       "      <td>116276.06</td>\n",
       "      <td>2262.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>114013.40</td>\n",
       "      <td>47346.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47346.73</td>\n",
       "      <td>114013.40</td>\n",
       "      <td>-1883610.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.118575</td>\n",
       "      <td>49.71</td>\n",
       "      <td>99.41</td>\n",
       "      <td>121312.08</td>\n",
       "      <td>2404.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66666.67</td>\n",
       "      <td>118907.11</td>\n",
       "      <td>52240.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52240.44</td>\n",
       "      <td>118907.11</td>\n",
       "      <td>-1764703.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  price_per_gpu_hr  util_pct  delivered_util_pct  revenue_usd  \\\n",
       "0      1          2.325000     35.00               70.00     93744.00   \n",
       "1      2          2.282165     37.94               75.88     99749.42   \n",
       "2      3          2.240120     40.88               81.76    105501.75   \n",
       "3      4          2.198849     43.82               87.65    111008.26   \n",
       "4      5          2.158339     46.76               93.53    116276.06   \n",
       "5      6          2.118575     49.71               99.41    121312.08   \n",
       "\n",
       "   power_cost_usd  other_opex_usd  depreciation_usd  ebitda_usd  ebit_usd  \\\n",
       "0         1693.44             0.0          66666.67    92050.56  25383.89   \n",
       "1         1835.75             0.0          66666.67    97913.68  31247.01   \n",
       "2         1978.05             0.0          66666.67   103523.70  36857.03   \n",
       "3         2120.36             0.0          66666.67   108887.90  42221.23   \n",
       "4         2262.66             0.0          66666.67   114013.40  47346.73   \n",
       "5         2404.97             0.0          66666.67   118907.11  52240.44   \n",
       "\n",
       "   tax_usd  net_income_usd  cashflow_usd  cum_cash_usd  \n",
       "0      0.0        25383.89   -2307949.44   -2307949.44  \n",
       "1      0.0        31247.01      97913.68   -2210035.76  \n",
       "2      0.0        36857.03     103523.70   -2106512.07  \n",
       "3      0.0        42221.23     108887.90   -1997624.17  \n",
       "4      0.0        47346.73     114013.40   -1883610.77  \n",
       "5      0.0        52240.44     118907.11   -1764703.66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {\n",
      "  \"gpu_model\": \"H100\",\n",
      "  \"region\": \"Global\",\n",
      "  \"fleet_gpus\": 80,\n",
      "  \"price_per_gpu_hr_initial\": 2.325,\n",
      "  \"oversub_multiplier\": 2.0,\n",
      "  \"price_erosion_yoy\": 0.2,\n",
      "  \"util_model\": \"linear\",\n",
      "  \"util_start_pct\": 0.35,\n",
      "  \"util_peak_pct\": 0.85,\n",
      "  \"util_months_to_peak\": 18,\n",
      "  \"watts_per_gpu\": 350,\n",
      "  \"usd_per_kwh\": 0.12,\n",
      "  \"other_opex_usd_per_month\": 0.0,\n",
      "  \"capex_usd\": 2400000,\n",
      "  \"capex_month0\": true,\n",
      "  \"depreciation_months\": 36,\n",
      "  \"tax_rate\": 0.0,\n",
      "  \"months\": 60,\n",
      "  \"payback_month\": 23,\n",
      "  \"irr_monthly\": 0.03261738789953898,\n",
      "  \"irr_annualized\": 0.4698506356943497\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === ROI CALCULATORS: tenant quotes + provider earnings =======================\n",
    "# Outputs:\n",
    "#   - docs/data/derived/tenant_roi_quotes.csv\n",
    "#   - docs/data/derived/provider_roi_projection.csv\n",
    "#   - docs/data/derived/provider_roi_summary.json\n",
    "#\n",
    "# Relies on provider prices in memory as `scored` OR docs/data/derived/provider_scores_latest.csv\n",
    "\n",
    "import json, math, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ---------------- basics / IO ----------------\n",
    "BASE = Path(\"docs/data\")\n",
    "DERIVED = BASE / \"derived\"\n",
    "DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        if np.isfinite(v): return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return default\n",
    "\n",
    "def to_hours(s):\n",
    "    if not isinstance(s,str) or not s.strip(): return np.nan\n",
    "    t = s.strip().lower().rstrip(\"s\")\n",
    "    m = __import__(\"re\").match(r\"^(\\d+(?:\\.\\d+)?)\\s*([a-z]+)$\", t)\n",
    "    if not m: return np.nan\n",
    "    q,u = float(m.group(1)), m.group(2)\n",
    "    mult = {\"h\":1,\"hr\":1,\"hour\":1,\"d\":24,\"day\":24,\"w\":168,\"wk\":168,\"week\":168,\"mo\":720,\"month\":720}.get(u, np.nan)\n",
    "    return q*mult\n",
    "\n",
    "# Ensure we have prices in a DataFrame named `scored`\n",
    "try:\n",
    "    assert isinstance(scored, pd.DataFrame) and not scored.empty\n",
    "except Exception:\n",
    "    prices_path = DERIVED / \"provider_scores_latest.csv\"\n",
    "    scored = pd.read_csv(prices_path, low_memory=False) if prices_path.exists() else pd.DataFrame()\n",
    "if scored.empty:\n",
    "    raise RuntimeError(\"No price table available (scored). Run the pricing pipeline first.\")\n",
    "\n",
    "# Make sure price column exists\n",
    "PRICE_COL = \"effective_price_usd_per_gpu_hr\"\n",
    "if PRICE_COL not in scored.columns:\n",
    "    raise RuntimeError(f\"Missing {PRICE_COL} in provider prices.\")\n",
    "\n",
    "# ---------------- A) TENANT ROI / COST QUOTES ----------------\n",
    "@dataclass\n",
    "class UsageInputs:\n",
    "    gpu_model: str       # \"H100\" | \"H200\"\n",
    "    region: str          # \"US\" | \"EU\" | \"Global\" ...\n",
    "    count: int           # number of GPUs\n",
    "    duration: str        # \"1 hour\" | \"1 day\" | \"1 week\" | \"1 month\"\n",
    "    utilization_pct: float = 100.0     # % of the time they’re busy\n",
    "    type: str = \"On-Demand\"\n",
    "    provider: str = \"All providers\"    # specific provider or \"All providers\"\n",
    "    use_predictions: bool = False      # if True, use group p50 price for cost\n",
    "\n",
    "# Helper: group median (p50) for a request (by gpu_model + region, falling back)\n",
    "def _group_p50(df: pd.DataFrame, gpu_model: str, region: str) -> float:\n",
    "    d = df[df[\"gpu_model\"].str.upper()==gpu_model.upper()]\n",
    "    if region and isinstance(region,str):\n",
    "        d_reg = d[d[\"region\"]==region]\n",
    "        if d_reg.empty:\n",
    "            d_reg = d[d[\"region\"].str.lower().eq(\"global\")]\n",
    "        if d_reg.empty:\n",
    "            d_reg = d\n",
    "    else:\n",
    "        d_reg = d\n",
    "    return float(d_reg[PRICE_COL].median()) if not d_reg.empty else np.nan\n",
    "\n",
    "def build_usage_quotes(inputs: UsageInputs, price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter rows\n",
    "    df = price_df.copy()\n",
    "    df = df[df[\"gpu_model\"].str.upper()==inputs.gpu_model.upper()]\n",
    "    df = df[df[\"type\"].str.contains(inputs.type, case=False, na=False)]\n",
    "    # region preference: exact -> Global -> any\n",
    "    df_reg = df[df[\"region\"]==inputs.region]\n",
    "    used_region = inputs.region\n",
    "    if df_reg.empty:\n",
    "        df_reg = df[df[\"region\"].str.lower().eq(\"global\")]\n",
    "        used_region = \"Global\"\n",
    "    if df_reg.empty:\n",
    "        df_reg = df\n",
    "        used_region = \"mixed\"\n",
    "\n",
    "    if inputs.provider and inputs.provider != \"All providers\":\n",
    "        df_reg = df_reg[df_reg[\"provider\"].str.strip().str.lower()==inputs.provider.strip().lower()]\n",
    "\n",
    "    if df_reg.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\"utilization_pct\",\n",
    "            \"price_per_gpu_hr\",\"total_cost_usd\",\"price_source\",\"source_url\",\"asof_utc\"\n",
    "        ])\n",
    "\n",
    "    hrs  = to_hours(inputs.duration)\n",
    "    util = max(0.0, min(100.0, float(inputs.utilization_pct))) / 100.0\n",
    "    cnt  = int(inputs.count)\n",
    "\n",
    "    # price selection\n",
    "    if inputs.use_predictions:\n",
    "        p50 = _group_p50(df_reg, inputs.gpu_model, used_region)\n",
    "        price_col = pd.Series([p50] * len(df_reg), index=df_reg.index)\n",
    "        src = \"pred_p50\"\n",
    "    else:\n",
    "        price_col = df_reg[PRICE_COL]\n",
    "        src = \"observed\"\n",
    "\n",
    "    out = df_reg.copy()\n",
    "    out[\"gpu_count\"]       = cnt\n",
    "    out[\"duration\"]        = inputs.duration\n",
    "    out[\"utilization_pct\"] = inputs.utilization_pct\n",
    "    out[\"price_per_gpu_hr\"]= price_col.astype(float)\n",
    "    out[\"total_cost_usd\"]  = (out[\"price_per_gpu_hr\"] * cnt * hrs * util).round(2)\n",
    "    out[\"price_source\"]    = src\n",
    "    out[\"asof_utc\"]        = _now_iso()\n",
    "\n",
    "    keep = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\"utilization_pct\",\n",
    "            \"price_per_gpu_hr\",\"total_cost_usd\",\"price_source\",\"source_url\",\"asof_utc\"]\n",
    "    # sort by total cost ascending\n",
    "    return out[keep].sort_values([\"total_cost_usd\",\"price_per_gpu_hr\",\"provider\"]).reset_index(drop=True)\n",
    "\n",
    "# Example inputs (hook your UI to these)\n",
    "usage = UsageInputs(\n",
    "    gpu_model=\"H100\",\n",
    "    region=\"US\",\n",
    "    count=8,\n",
    "    duration=\"1 week\",\n",
    "    utilization_pct=100,\n",
    "    type=\"On-Demand\",\n",
    "    provider=\"All providers\",\n",
    "    use_predictions=False,      # flip to True if you want p50-based “market” quote\n",
    ")\n",
    "\n",
    "tenant_quotes = build_usage_quotes(usage, scored)\n",
    "tenant_path = DERIVED / \"tenant_roi_quotes.csv\"\n",
    "tenant_quotes.to_csv(tenant_path, index=False)\n",
    "print(f\"✔ wrote {tenant_path} ({len(tenant_quotes)} rows)\")\n",
    "display(tenant_quotes.head(10))\n",
    "\n",
    "# ---------------- B) PROVIDER ROI / EARNINGS PROJECTION ----------------\n",
    "@dataclass\n",
    "class ROIInputs:\n",
    "    gpu_model: str = \"H100\"\n",
    "    region: str = \"Global\"\n",
    "    fleet_gpus: int = 80                 # physical GPUs\n",
    "    price_per_gpu_hr_initial: float = 1.25\n",
    "    oversub_multiplier: float = 2.0      # effective util = min(util * oversub, 0.99)\n",
    "    price_erosion_yoy: float = 0.20      # 20% YoY decline\n",
    "    util_model: str = \"linear\"           # \"linear\" or \"logistic\"\n",
    "    util_start_pct: float = 0.35\n",
    "    util_peak_pct: float = 0.85\n",
    "    util_months_to_peak: int = 18\n",
    "    watts_per_gpu: float = 350.0\n",
    "    usd_per_kwh: float = 0.12\n",
    "    other_opex_usd_per_month: float = 0.0\n",
    "    capex_usd: float = 2_400_000.0\n",
    "    capex_month0: bool = True\n",
    "    depreciation_months: int = 36\n",
    "    tax_rate: float = 0.0                # simple EBIT tax\n",
    "    months: int = 60                     # projection horizon\n",
    "\n",
    "def _price_erosion_path(p0: float, months: int, yoy: float) -> np.ndarray:\n",
    "    # Convert YoY decline to monthly factor\n",
    "    m = 1.0 - (1.0 - yoy) ** (1.0 / 12.0)\n",
    "    return p0 * (1.0 - m) ** np.arange(months)\n",
    "\n",
    "def _util_path_linear(start: float, peak: float, months_to_peak: int, n: int) -> np.ndarray:\n",
    "    x = np.linspace(0, 1, max(2, months_to_peak))\n",
    "    path = np.interp(np.arange(n), np.arange(months_to_peak), start + (peak - start) * x)\n",
    "    path = np.clip(path, min(start,peak), max(start,peak))\n",
    "    if n > months_to_peak:\n",
    "        tail = np.full(n - months_to_peak, peak)\n",
    "        path = np.concatenate([path[:months_to_peak], tail])\n",
    "    return path\n",
    "\n",
    "def _util_path_logistic(start: float, peak: float, months_to_peak: int, n: int) -> np.ndarray:\n",
    "    # Simple logistic reaching ~peak at months_to_peak\n",
    "    t = np.arange(n)\n",
    "    L = peak\n",
    "    k = 6.0 / max(1, months_to_peak)      # controls steepness\n",
    "    x0 = months_to_peak / 2.0\n",
    "    y = L / (1.0 + np.exp(-k * (t - x0)))\n",
    "    # shift so y(0)=start approximately\n",
    "    adj = (y[0] if n>0 else 0.0)\n",
    "    if adj != 0:\n",
    "        y = (y - adj) * ((peak - start) / (y[-1] - adj + 1e-9)) + start\n",
    "    return np.clip(y, 0.0, peak)\n",
    "\n",
    "def _irr(cashflows: np.ndarray, guess: float = 0.02, max_iter: int = 100, tol: float = 1e-7) -> float:\n",
    "    # Secant method IRR to avoid numpy_financial dependency\n",
    "    def npv(rate):\n",
    "        return np.sum(cashflows / (1.0 + rate) ** np.arange(len(cashflows)))\n",
    "    r0, r1 = guess, guess * 1.1 + 1e-6\n",
    "    f0, f1 = npv(r0), npv(r1)\n",
    "    for _ in range(max_iter):\n",
    "        if abs(f1 - f0) < 1e-12: break\n",
    "        r2 = r1 - f1 * (r1 - r0) / (f1 - f0)\n",
    "        if not np.isfinite(r2) or r2 <= -0.9999: r2 = 0.0\n",
    "        f2 = npv(r2)\n",
    "        if abs(f2) < tol: return r2\n",
    "        r0, f0, r1, f1 = r1, f1, r2, f2\n",
    "    return r1\n",
    "\n",
    "def project_provider(inputs: ROIInputs) -> Tuple[pd.DataFrame, Dict]:\n",
    "    n = int(inputs.months)\n",
    "    # price and utilization paths\n",
    "    price = _price_erosion_path(inputs.price_per_gpu_hr_initial, n, inputs.price_erosion_yoy)\n",
    "    if inputs.util_model.lower().startswith(\"log\"):\n",
    "        util = _util_path_logistic(inputs.util_start_pct, inputs.util_peak_pct,\n",
    "                                   max(1, inputs.util_months_to_peak), n)\n",
    "    else:\n",
    "        util = _util_path_linear(inputs.util_start_pct, inputs.util_peak_pct,\n",
    "                                 max(1, inputs.util_months_to_peak), n)\n",
    "    util = np.clip(util, 0.0, 0.999)\n",
    "\n",
    "    # ✅ FIX: provide both a_min and a_max\n",
    "    delivered_util = np.clip(util * max(0.0, float(inputs.oversub_multiplier)), 0.0, 0.999)\n",
    "\n",
    "    # monthly hours\n",
    "    H = 720.0  # 30 days\n",
    "    # revenues\n",
    "    gpu_hours = inputs.fleet_gpus * delivered_util * H\n",
    "    revenue  = gpu_hours * price\n",
    "\n",
    "    # power costs (scale with delivered utilization)\n",
    "    kwh_per_gpu_month = (inputs.watts_per_gpu / 1000.0) * 24.0 * 30.0 * delivered_util\n",
    "    power_cost = inputs.fleet_gpus * kwh_per_gpu_month * inputs.usd_per_kwh\n",
    "\n",
    "    other_opex = np.full(n, float(inputs.other_opex_usd_per_month))\n",
    "    depreciation = np.zeros(n)\n",
    "    if inputs.depreciation_months > 0:\n",
    "        dep = inputs.capex_usd / inputs.depreciation_months\n",
    "        depreciation[:inputs.depreciation_months] = dep\n",
    "\n",
    "    ebitda = revenue - power_cost - other_opex\n",
    "    ebit   = ebitda - depreciation\n",
    "    tax    = np.where(ebit > 0, inputs.tax_rate * ebit, 0.0)\n",
    "    net_income = ebit - tax\n",
    "\n",
    "    # cash flow: EBITDA - tax - initial capex (if month0)\n",
    "    cashflow = ebitda - tax\n",
    "    if inputs.capex_month0 and inputs.capex_usd > 0:\n",
    "        cashflow[0] -= inputs.capex_usd\n",
    "    cum_cash = np.cumsum(cashflow)\n",
    "\n",
    "    # KPIs\n",
    "    payback_month = int(np.argmax(cum_cash >= 0)) if np.any(cum_cash >= 0) else None\n",
    "    irr_m = _irr(cashflow)  # monthly IRR\n",
    "    irr_ann = (1.0 + irr_m) ** 12 - 1.0 if np.isfinite(irr_m) else np.nan\n",
    "\n",
    "    months = np.arange(1, n+1)\n",
    "    df = pd.DataFrame({\n",
    "        \"month\": months,\n",
    "        \"price_per_gpu_hr\": price.round(6),\n",
    "        \"util_pct\": (util*100).round(2),\n",
    "        \"delivered_util_pct\": (delivered_util*100).round(2),\n",
    "        \"revenue_usd\": revenue.round(2),\n",
    "        \"power_cost_usd\": power_cost.round(2),\n",
    "        \"other_opex_usd\": other_opex.round(2),\n",
    "        \"depreciation_usd\": depreciation.round(2),\n",
    "        \"ebitda_usd\": ebitda.round(2),\n",
    "        \"ebit_usd\": ebit.round(2),\n",
    "        \"tax_usd\": tax.round(2),\n",
    "        \"net_income_usd\": net_income.round(2),\n",
    "        \"cashflow_usd\": cashflow.round(2),\n",
    "        \"cum_cash_usd\": cum_cash.round(2),\n",
    "    })\n",
    "\n",
    "    summary = {\n",
    "        \"gpu_model\": inputs.gpu_model,\n",
    "        \"region\": inputs.region,\n",
    "        \"fleet_gpus\": inputs.fleet_gpus,\n",
    "        \"price_per_gpu_hr_initial\": inputs.price_per_gpu_hr_initial,\n",
    "        \"oversub_multiplier\": inputs.oversub_multiplier,\n",
    "        \"price_erosion_yoy\": inputs.price_erosion_yoy,\n",
    "        \"util_model\": inputs.util_model,\n",
    "        \"util_start_pct\": inputs.util_start_pct,\n",
    "        \"util_peak_pct\": inputs.util_peak_pct,\n",
    "        \"util_months_to_peak\": inputs.util_months_to_peak,\n",
    "        \"watts_per_gpu\": inputs.watts_per_gpu,\n",
    "        \"usd_per_kwh\": inputs.usd_per_kwh,\n",
    "        \"other_opex_usd_per_month\": inputs.other_opex_usd_per_month,\n",
    "        \"capex_usd\": inputs.capex_usd,\n",
    "        \"capex_month0\": inputs.capex_month0,\n",
    "        \"depreciation_months\": inputs.depreciation_months,\n",
    "        \"tax_rate\": inputs.tax_rate,\n",
    "        \"months\": n,\n",
    "        \"payback_month\": payback_month,\n",
    "        \"irr_monthly\": float(irr_m) if np.isfinite(irr_m) else None,\n",
    "        \"irr_annualized\": float(irr_ann) if np.isfinite(irr_ann) else None,\n",
    "    }\n",
    "    return df, summary\n",
    "\n",
    "\n",
    "# Example provider ROI inputs (wire these to your dashboard controls)\n",
    "provider_inputs = ROIInputs(\n",
    "    gpu_model=\"H100\",\n",
    "    region=\"Global\",\n",
    "    fleet_gpus=80,\n",
    "    price_per_gpu_hr_initial= _safe_float(scored[scored[\"gpu_model\"]==\"H100\"][PRICE_COL].median(), 1.25),\n",
    "    oversub_multiplier=2.0,\n",
    "    price_erosion_yoy=0.20,\n",
    "    util_model=\"linear\",\n",
    "    util_start_pct=0.35,\n",
    "    util_peak_pct=0.85,\n",
    "    util_months_to_peak=18,\n",
    "    watts_per_gpu=350,\n",
    "    usd_per_kwh=0.12,\n",
    "    other_opex_usd_per_month=0.0,\n",
    "    capex_usd=2_400_000,\n",
    "    capex_month0=True,\n",
    "    depreciation_months=36,\n",
    "    tax_rate=0.0,\n",
    "    months=60,\n",
    ")\n",
    "\n",
    "provider_monthly, provider_summary = project_provider(provider_inputs)\n",
    "prov_path = DERIVED / \"provider_roi_projection.csv\"\n",
    "provider_monthly.to_csv(prov_path, index=False)\n",
    "(DERIVED / \"provider_roi_summary.json\").write_text(json.dumps(provider_summary, indent=2))\n",
    "print(f\"✔ wrote {prov_path} ({len(provider_monthly)} rows)\")\n",
    "print(f\"✔ wrote {DERIVED / 'provider_roi_summary.json'}\")\n",
    "display(provider_monthly.head(6))\n",
    "print(\"Summary:\", json.dumps(provider_summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ wrote docs/data/derived/compute_calculator_snapshot.csv (rows: 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider_type</th>\n",
       "      <th>$/GPU-hr</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>price_lo</th>\n",
       "      <th>price_md</th>\n",
       "      <th>price_hi</th>\n",
       "      <th>term_hours</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>used_region</th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.99–$1.99</td>\n",
       "      <td>$68,774–$68,774</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.990</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>Neocloud</td>\n",
       "      <td>$1.99–$1.99</td>\n",
       "      <td>$68,774–$68,774</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.990</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100+H200</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.38–$2.18</td>\n",
       "      <td>$47,692–$75,202</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.176</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H200</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.38–$2.03</td>\n",
       "      <td>$47,692–$70,156</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H200</td>\n",
       "      <td>Neocloud</td>\n",
       "      <td>$1.38–$2.03</td>\n",
       "      <td>$47,692–$70,156</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_model provider_type     $/GPU-hr            TOTAL  price_lo  price_md  \\\n",
       "0       H100           All  $1.99–$1.99  $68,774–$68,774      1.99      1.99   \n",
       "1       H100      Neocloud  $1.99–$1.99  $68,774–$68,774      1.99      1.99   \n",
       "2  H100+H200           All  $1.38–$2.18  $47,692–$75,202      1.38      1.40   \n",
       "3       H200           All  $1.38–$2.03  $47,692–$70,156      1.38      1.39   \n",
       "4       H200      Neocloud  $1.38–$2.03  $47,692–$70,156      1.38      1.39   \n",
       "\n",
       "   price_hi  term_hours  gpu_count used_region  n_quotes  source  \n",
       "0     1.990        4320          8          US         1  latest  \n",
       "1     1.990        4320          8          US         1  latest  \n",
       "2     2.176        4320          8          US         4  latest  \n",
       "3     2.030        4320          8          US         3  latest  \n",
       "4     2.030        4320          8          US         3  latest  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Compute calculator snapshot: bands per provider_type & GPU model ===\n",
    "import numpy as np, pandas as pd, math, re\n",
    "from pathlib import Path\n",
    "\n",
    "DERIVED = Path(\"docs/data/derived\")\n",
    "DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- provider typing (same mapping you used elsewhere) ----\n",
    "_HYPERSCALER = {\"AWS\",\"Amazon\",\"GCP\",\"Google Cloud\",\"Azure\",\"Microsoft Azure\",\"OCI\",\"Oracle Cloud\"}\n",
    "_BROKER      = {\"Vast.ai\",\"Hydra Host (Brokkr)\",\"Brokkr\",\"TensorDock\",\"Shadeform\"}\n",
    "_NEO         = {\"VoltagePark\",\"CoreWeave\",\"Lambda Labs\",\"RunPod\",\"Runpod\",\"Paperspace\",\"Nebius\",\"SFCompute\",\"OVHcloud\"}\n",
    "OVERRIDES    = {\n",
    "    # \"RunPod\": \"Broker\",    # uncomment to re-bucket RunPod\n",
    "    # \"Runpod\": \"Broker\",\n",
    "}\n",
    "\n",
    "def infer_provider_type(name: str) -> str:\n",
    "    n = str(name or \"\").strip()\n",
    "    if n in OVERRIDES: return OVERRIDES[n]\n",
    "    if n in _HYPERSCALER: return \"Hyperscaler\"\n",
    "    if n in _BROKER:      return \"Broker\"\n",
    "    if n in _NEO:         return \"Neocloud\"\n",
    "    u = n.lower()\n",
    "    if any(k in u for k in [\"aws\",\"amazon\",\"gcp\",\"google\",\"azure\",\"oci\",\"oracle\"]): return \"Hyperscaler\"\n",
    "    if any(k in u for k in [\"vast\",\"brokkr\",\"hydra\",\"tensordock\",\"shadeform\"]):     return \"Broker\"\n",
    "    return \"Neocloud\"\n",
    "\n",
    "# ---- term hours helper ----\n",
    "def hours_for_term(term: str) -> int:\n",
    "    t = str(term).strip().lower()\n",
    "    if t in {\"1 hour\",\"1h\",\"hour\"}:   return 1\n",
    "    if t in {\"1 day\",\"1d\",\"day\"}:     return 24\n",
    "    if t in {\"1 week\",\"1w\",\"week\"}:   return 7*24\n",
    "    if t in {\"1 month\",\"1mo\",\"month\"}:return 30*24\n",
    "    # “6 months”, “3 months”, etc.\n",
    "    m = re.match(r\"(\\d+)\\s*month\", t)\n",
    "    if m: return int(m.group(1))*30*24\n",
    "    raise ValueError(f\"Unrecognised term: {term}\")\n",
    "\n",
    "# ---- weighted quantiles for bands ----\n",
    "def weighted_quantiles(vals, weights, qs=(0.10,0.50,0.90)):\n",
    "    v = np.asarray(vals, dtype=float)\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    m = np.isfinite(v) & np.isfinite(w) & (w>=0)\n",
    "    v, w = v[m], w[m]\n",
    "    if v.size == 0 or w.sum()==0:\n",
    "        return {q: np.nan for q in qs}\n",
    "    order = np.argsort(v)\n",
    "    v, w = v[order], w[order]\n",
    "    c = np.cumsum(w)\n",
    "    tot = c[-1]\n",
    "    return {q: float(np.interp(q*tot, c, v)) for q in qs}\n",
    "\n",
    "# ---- similarity weight (for future: VRAM/CPU if present; now uniform) ----\n",
    "def row_weight(_row):  # keep 1.0 for now; expand later when VRAM/CPU attrs are populated\n",
    "    return 1.0\n",
    "\n",
    "# ---- band computation for a subset ----\n",
    "def band_for_subset(df_sub: pd.DataFrame, gpu_count: int, term_hours: int):\n",
    "    if df_sub.empty:\n",
    "        return None\n",
    "    w = df_sub.apply(row_weight, axis=1).values\n",
    "    qs = weighted_quantiles(df_sub[\"effective_price_usd_per_gpu_hr\"].values, w, qs=(0.10,0.50,0.90))\n",
    "    price_lo = qs[0.10]; price_md = qs[0.50]; price_hi = qs[0.90]\n",
    "    total_lo = price_lo * gpu_count * term_hours if np.isfinite(price_lo) else np.nan\n",
    "    total_md = price_md * gpu_count * term_hours if np.isfinite(price_md) else np.nan\n",
    "    total_hi = price_hi * gpu_count * term_hours if np.isfinite(price_hi) else np.nan\n",
    "    return dict(\n",
    "        p10=round(price_lo,4) if np.isfinite(price_lo) else None,\n",
    "        p50=round(price_md,4) if np.isfinite(price_md) else None,\n",
    "        p90=round(price_hi,4) if np.isfinite(price_hi) else None,\n",
    "        total_lo=round(total_lo,2) if np.isfinite(total_lo) else None,\n",
    "        total_p50=round(total_md,2) if np.isfinite(total_md) else None,\n",
    "        total_hi=round(total_hi,2) if np.isfinite(total_hi) else None,\n",
    "        n=int(len(df_sub))\n",
    "    )\n",
    "\n",
    "# ---- main snapshot builder ----\n",
    "def compute_calculator_snapshot(\n",
    "    priced_df: pd.DataFrame,\n",
    "    region: str = \"US\",\n",
    "    term: str = \"6 months\",\n",
    "    gpu_count: int = 8,\n",
    "    allowed_types=(\"On-Demand\",),          # only compare On-Demand by default\n",
    "    include_overall=True                   # add Overall (H100+H200) row\n",
    ") -> pd.DataFrame:\n",
    "    # basic hygiene\n",
    "    df = priced_df.copy()\n",
    "    df = df[df[\"effective_price_usd_per_gpu_hr\"].notna()]\n",
    "    df[\"provider_type\"] = df[\"provider\"].map(infer_provider_type)\n",
    "\n",
    "    term_hours = hours_for_term(term)\n",
    "\n",
    "    rows = []\n",
    "    for model in [\"H100\",\"H200\"]:\n",
    "        base = df[df[\"gpu_model\"].eq(model)]\n",
    "        if base.empty:\n",
    "            continue\n",
    "\n",
    "        # type filter\n",
    "        if allowed_types:\n",
    "            base = base[base[\"type\"].isin(allowed_types)]\n",
    "\n",
    "        # region preference: exact -> Global -> mixed\n",
    "        pref = base[base[\"region\"].eq(region)]\n",
    "        used_region = region\n",
    "        if pref.empty:\n",
    "            pref = base[base[\"region\"].str.lower().eq(\"global\")]\n",
    "            used_region = \"Global\"\n",
    "        if pref.empty:\n",
    "            pref = base\n",
    "            used_region = \"mixed\"\n",
    "\n",
    "        # one row per provider_type\n",
    "        for ptype, grp in pref.groupby(\"provider_type\", dropna=False):\n",
    "            band = band_for_subset(grp, gpu_count, term_hours)\n",
    "            if not band:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"gpu_model\": model,\n",
    "                \"provider_type\": ptype or \"Unknown\",\n",
    "                \"$/GPU-hr\": f\"${band['p10']:.2f}–${band['p90']:.2f}\" if band[\"p10\"] is not None else None,\n",
    "                \"TOTAL\":    f\"${int(band['total_lo']):,}–${int(band['total_hi']):,}\" if band[\"total_lo\"] is not None else None,\n",
    "                \"price_lo\": band[\"p10\"], \"price_md\": band[\"p50\"], \"price_hi\": band[\"p90\"],\n",
    "                \"term_hours\": term_hours, \"gpu_count\": gpu_count,\n",
    "                \"used_region\": used_region, \"n_quotes\": band[\"n\"], \"source\": \"latest\"\n",
    "            })\n",
    "\n",
    "        # market average (all provider types) for this model\n",
    "        band_all = band_for_subset(pref, gpu_count, term_hours)\n",
    "        if band_all:\n",
    "            rows.append({\n",
    "                \"gpu_model\": model,\n",
    "                \"provider_type\": \"All\",\n",
    "                \"$/GPU-hr\": f\"${band_all['p10']:.2f}–${band_all['p90']:.2f}\" if band_all[\"p10\"] is not None else None,\n",
    "                \"TOTAL\":    f\"${int(band_all['total_lo']):,}–${int(band_all['total_hi']):,}\" if band_all[\"total_lo\"] is not None else None,\n",
    "                \"price_lo\": band_all[\"p10\"], \"price_md\": band_all[\"p50\"], \"price_hi\": band_all[\"p90\"],\n",
    "                \"term_hours\": term_hours, \"gpu_count\": gpu_count,\n",
    "                \"used_region\": used_region, \"n_quotes\": band_all[\"n\"], \"source\": \"latest\"\n",
    "            })\n",
    "\n",
    "    # optional overall band across both H100+H200 (uses the same regional preference)\n",
    "    if include_overall:\n",
    "        base = df[df[\"type\"].isin(allowed_types)]\n",
    "        pref = base[base[\"region\"].eq(region)]\n",
    "        used_region = region\n",
    "        if pref.empty:\n",
    "            pref = base[base[\"region\"].str.lower().eq(\"global\")]\n",
    "            used_region = \"Global\"\n",
    "        if pref.empty:\n",
    "            pref = base\n",
    "            used_region = \"mixed\"\n",
    "        band_overall = band_for_subset(pref, gpu_count, term_hours)\n",
    "        if band_overall:\n",
    "            rows.append({\n",
    "                \"gpu_model\": \"H100+H200\",\n",
    "                \"provider_type\": \"All\",\n",
    "                \"$/GPU-hr\": f\"${band_overall['p10']:.2f}–${band_overall['p90']:.2f}\" if band_overall[\"p10\"] is not None else None,\n",
    "                \"TOTAL\":    f\"${int(band_overall['total_lo']):,}–${int(band_overall['total_hi']):,}\" if band_overall[\"total_lo\"] is not None else None,\n",
    "                \"price_lo\": band_overall[\"p10\"], \"price_md\": band_overall[\"p50\"], \"price_hi\": band_overall[\"p90\"],\n",
    "                \"term_hours\": term_hours, \"gpu_count\": gpu_count,\n",
    "                \"used_region\": used_region, \"n_quotes\": band_overall[\"n\"], \"source\": \"latest\"\n",
    "            })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values([\"gpu_model\",\"provider_type\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---- build and save snapshot (example: US, 6 months, 8 GPUs) ----\n",
    "calc_df = compute_calculator_snapshot(\n",
    "    scored,            # from your earlier pipeline\n",
    "    region=\"US\",\n",
    "    term=\"6 months\",\n",
    "    gpu_count=8,\n",
    "    allowed_types=(\"On-Demand\",),   # stick to On-Demand comparisons\n",
    "    include_overall=True\n",
    ")\n",
    "snap_path = DERIVED / \"compute_calculator_snapshot.csv\"\n",
    "calc_df.to_csv(snap_path, index=False)\n",
    "print(f\"✓ wrote {snap_path} (rows: {len(calc_df)})\")\n",
    "display(calc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ wrote docs/data/derived/compute_calculator_snapshot.csv (rows: 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider_type</th>\n",
       "      <th>$/GPU-hr</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>price_lo</th>\n",
       "      <th>price_md</th>\n",
       "      <th>price_hi</th>\n",
       "      <th>term_hours</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>used_region</th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.99–$2.99</td>\n",
       "      <td>$68774–$103421</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>11</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>Neocloud</td>\n",
       "      <td>$1.99–$3.12</td>\n",
       "      <td>$68774–$108000</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.12</td>\n",
       "      <td>3.12</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>6</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>Broker</td>\n",
       "      <td>$1.25–$2.30</td>\n",
       "      <td>$43200–$79488</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.30</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>Hyperscaler</td>\n",
       "      <td>$2.99–$2.99</td>\n",
       "      <td>$103334–$103421</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H200</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.39–$3.22</td>\n",
       "      <td>$48108–$111145</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.30</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>7</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H200</td>\n",
       "      <td>Neocloud</td>\n",
       "      <td>$1.39–$3.37</td>\n",
       "      <td>$48038–$116467</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.30</td>\n",
       "      <td>3.37</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>6</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H200</td>\n",
       "      <td>Broker</td>\n",
       "      <td>$2.50</td>\n",
       "      <td>$86400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100+H200</td>\n",
       "      <td>All</td>\n",
       "      <td>$1.39–$3.26</td>\n",
       "      <td>$48177–$112830</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.30</td>\n",
       "      <td>3.26</td>\n",
       "      <td>4320</td>\n",
       "      <td>8</td>\n",
       "      <td>US</td>\n",
       "      <td>18</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_model provider_type     $/GPU-hr            TOTAL  price_lo  price_md  \\\n",
       "0       H100           All  $1.99–$2.99   $68774–$103421      1.99      2.25   \n",
       "1       H100      Neocloud  $1.99–$3.12   $68774–$108000      1.99      2.12   \n",
       "2       H100        Broker  $1.25–$2.30    $43200–$79488      1.25      2.25   \n",
       "3       H100   Hyperscaler  $2.99–$2.99  $103334–$103421      2.99      2.99   \n",
       "4       H200           All  $1.39–$3.22   $48108–$111145      1.39      2.30   \n",
       "5       H200      Neocloud  $1.39–$3.37   $48038–$116467      1.39      2.30   \n",
       "6       H200        Broker        $2.50           $86400      2.50      2.50   \n",
       "7  H100+H200           All  $1.39–$3.26   $48177–$112830      1.39      2.30   \n",
       "\n",
       "   price_hi  term_hours  gpu_count used_region  n_quotes  source  \n",
       "0      2.99        4320          8          US        11  latest  \n",
       "1      3.12        4320          8          US         6  latest  \n",
       "2      2.30        4320          8          US         3  latest  \n",
       "3      2.99        4320          8          US         2  latest  \n",
       "4      3.22        4320          8          US         7  latest  \n",
       "5      3.37        4320          8          US         6  latest  \n",
       "6      2.50        4320          8          US         1  latest  \n",
       "7      3.26        4320          8          US        18  latest  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Compute Calculator snapshot (provider buckets + robust region fallback) ----------\n",
    "# Depends on: PRICE_COL, ensure_price_col(), _exclude_providers(), od (On-Demand pool)\n",
    "\n",
    "# 1) Provider bucket classifier (keep names you actually scrape)\n",
    "PROVIDER_TYPE_MAP = {\n",
    "    # Neoclouds\n",
    "    \"Lambda Labs\": \"Neocloud\",\n",
    "    \"VoltagePark\": \"Neocloud\",\n",
    "    \"Paperspace\":  \"Neocloud\",\n",
    "    \"Shadeform\":   \"Neocloud\",\n",
    "    \"Nebius\":      \"Neocloud\",\n",
    "    \"CrusoeCloud\": \"Neocloud\",\n",
    "    \"SFCompute\":   \"Neocloud\",\n",
    "    \"OVHcloud\":    \"Hyperscaler\",   # use as hyperscaler stand-in until AWS/GCP/Azure scrapers exist\n",
    "    # Brokers / Marketplaces\n",
    "    \"Vast.ai\":     \"Broker\",\n",
    "    \"RunPod\":      \"Broker\",\n",
    "    \"TensorDock\":  \"Broker\",\n",
    "    \"Hydra Host (Brokkr)\": \"Broker\",\n",
    "}\n",
    "\n",
    "def classify_provider_type(name: str) -> str:\n",
    "    n = str(name or \"\").strip()\n",
    "    if n in PROVIDER_TYPE_MAP:\n",
    "        return PROVIDER_TYPE_MAP[n]\n",
    "    # fallbacks\n",
    "    if re.search(r\"vast|runpod|tensor|brokkr|broker\", n, flags=re.I): return \"Broker\"\n",
    "    if re.search(r\"aws|azure|gcp|google|ovh|oci|oracle\", n, flags=re.I): return \"Hyperscaler\"\n",
    "    return \"Neocloud\"\n",
    "\n",
    "# 2) Prep dataset (US with Global fallback; exclude CoreWeave)\n",
    "def _region_pool(df: pd.DataFrame, prefer_region=\"US\", min_quotes=8) -> (pd.DataFrame, str):\n",
    "    us  = df[df[\"region\"] == prefer_region]\n",
    "    glb = df[df[\"region\"].str.lower().eq(\"global\")]\n",
    "    if len(us) >= min_quotes:\n",
    "        return us.copy(), prefer_region\n",
    "    # union (not replace) to keep any US quotes\n",
    "    pool = pd.concat([us, glb], ignore_index=True) if len(glb) else us\n",
    "    used = prefer_region if len(us) else (\"Global\" if len(glb) else \"mixed\")\n",
    "    return pool, used\n",
    "\n",
    "base = od.copy()\n",
    "base = _exclude_providers(base)\n",
    "base = ensure_price_col(base)\n",
    "base[\"provider_type\"] = base[\"provider\"].map(classify_provider_type)\n",
    "\n",
    "# 3) Stats per bucket\n",
    "def _bucket_stats(df: pd.DataFrame, term_months=6, gpu_count=8):\n",
    "    term_hours = int(term_months * 30 * 24)      # 6 months ≈ 4320h\n",
    "    vals = df[PRICE_COL].astype(float).dropna().sort_values()\n",
    "    n = len(vals)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    # robust quantiles (fallback for tiny samples)\n",
    "    p10 = float(vals.quantile(0.10)) if n >= 5 else float(vals.min())\n",
    "    p50 = float(vals.quantile(0.50)) if n >= 2 else float(vals.iloc[0])\n",
    "    p90 = float(vals.quantile(0.90)) if n >= 5 else float(vals.max())\n",
    "    def _fmt_range(lo, hi):\n",
    "        if not np.isfinite(lo) or not np.isfinite(hi): return \"—\"\n",
    "        if abs(lo - hi) < 1e-6: return f\"${lo:.2f}\"\n",
    "        return f\"${lo:.2f}–${hi:.2f}\"\n",
    "    price_str = _fmt_range(p10, p90)\n",
    "    total_lo  = p10 * gpu_count * term_hours\n",
    "    total_hi  = p90 * gpu_count * term_hours\n",
    "    total_str = _fmt_range(round(total_lo), round(total_hi)).replace(\".00\", \"\")\n",
    "    return {\n",
    "        \"$/GPU-hr\": price_str,\n",
    "        \"TOTAL\": total_str,\n",
    "        \"price_lo\": round(p10, 2),\n",
    "        \"price_md\": round(p50, 2),\n",
    "        \"price_hi\": round(p90, 2),\n",
    "        \"term_hours\": term_hours,\n",
    "        \"gpu_count\": int(gpu_count),\n",
    "        \"n_quotes\": int(n),\n",
    "        \"source\": \"latest\",\n",
    "    }\n",
    "\n",
    "def make_snapshot_rows(pool_df: pd.DataFrame, used_region: str, gpu_model_label: str):\n",
    "    rows = []\n",
    "    # ALL\n",
    "    stats_all = _bucket_stats(pool_df)\n",
    "    if stats_all:\n",
    "        rows.append({\"gpu_model\": gpu_model_label, \"provider_type\": \"All\",\n",
    "                     \"used_region\": used_region, **stats_all})\n",
    "    # Neocloud\n",
    "    st = _bucket_stats(pool_df[pool_df[\"provider_type\"]==\"Neocloud\"])\n",
    "    if st:\n",
    "        rows.append({\"gpu_model\": gpu_model_label, \"provider_type\": \"Neocloud\",\n",
    "                     \"used_region\": used_region, **st})\n",
    "    # Broker\n",
    "    st = _bucket_stats(pool_df[pool_df[\"provider_type\"]==\"Broker\"])\n",
    "    if st:\n",
    "        rows.append({\"gpu_model\": gpu_model_label, \"provider_type\": \"Broker\",\n",
    "                     \"used_region\": used_region, **st})\n",
    "    # Hyperscaler (will be filled by OVHcloud until AWS/GCP/Azure scrapers are added)\n",
    "    st = _bucket_stats(pool_df[pool_df[\"provider_type\"]==\"Hyperscaler\"])\n",
    "    if st:\n",
    "        rows.append({\"gpu_model\": gpu_model_label, \"provider_type\": \"Hyperscaler\",\n",
    "                     \"used_region\": used_region, **st})\n",
    "    return rows\n",
    "\n",
    "snap_rows = []\n",
    "\n",
    "# H100\n",
    "h100 = base[base[\"gpu_model\"]==\"H100\"]\n",
    "h100_pool, reg_h100 = _region_pool(h100, prefer_region=\"US\", min_quotes=8)\n",
    "snap_rows += make_snapshot_rows(h100_pool, reg_h100, \"H100\")\n",
    "\n",
    "# H200\n",
    "h200 = base[base[\"gpu_model\"]==\"H200\"]\n",
    "h200_pool, reg_h200 = _region_pool(h200, prefer_region=\"US\", min_quotes=8)\n",
    "snap_rows += make_snapshot_rows(h200_pool, reg_h200, \"H200\")\n",
    "\n",
    "# H100+H200 combined (All only)\n",
    "both_pool, reg_both = _region_pool(base, prefer_region=\"US\", min_quotes=12)\n",
    "st = _bucket_stats(both_pool)\n",
    "if st:\n",
    "    snap_rows.append({\"gpu_model\": \"H100+H200\", \"provider_type\": \"All\",\n",
    "                      \"used_region\": reg_both, **st})\n",
    "\n",
    "snap_df = (pd.DataFrame(snap_rows)\n",
    "             .loc[:, [\"gpu_model\",\"provider_type\",\"$/GPU-hr\",\"TOTAL\",\n",
    "                      \"price_lo\",\"price_md\",\"price_hi\",\n",
    "                      \"term_hours\",\"gpu_count\",\"used_region\",\"n_quotes\",\"source\"]])\n",
    "\n",
    "snap_path = DERIVED_DIR / \"compute_calculator_snapshot.csv\"\n",
    "snap_df.to_csv(snap_path, index=False)\n",
    "print(f\"✓ wrote {snap_path} (rows: {len(snap_df)})\")\n",
    "display(snap_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
